{"cells":[{"cell_type":"markdown","metadata":{"id":"jVGOHFL3W5hp"},"source":["## **LATAM Challenge - Data Ingest, Storage and Processing with Google Drive, Google Cloud Storage and Google BigQuery in Google Colab (Jupyter) with Python 3.10**\n","\n","**Welcome to the Data Engineer Challenge.** On this occasion, you will have the opportunity to get closer to the reality of the role, demonstrate your skills and knowledge in data processing with Python and different data structures.\n","\n","**Preparation:**\n","\n","1. Initial project commit (done with GitHub desktop)\n","2. Install Git Flow with `brew install git-flow`\n","3. Configure the repository with `git flow init`\n","4. Configure feature finishes to be done only in develop with `git config gitflow.feature.finish.keepremote true`\n","5. Synchronize the repository with GDrive\n","6. Read the code from GDrive with Colab\n","\n","**Additional Notes:**\n","\n","* The `README.md` file mentions using GitHub Desktop, Git Flow, and Colab. These are tools that can be used for version control and code collaboration.\n","* The `README.md` file also mentions measuring time and memory. This can be done using Python's built-in `time` and `memory_profiler` modules.\n","* English was used for both documentation and code."]},{"cell_type":"markdown","source":["## **Challenge Guidelines:**\n","\n","**Repository:**\n","\n","* Your solution must be in a public repository on the GitHub platform.\n","\n","**Submitting your challenge:**\n","\n","1. Make a POST request to [https://advana-challenge-check-api-cr-k4hdbggvoq-uc.a.run.app/data-engineer](https://advana-challenge-check-api-cr-k4hdbggvoq-uc.a.run.app/data-engineer).\n","2. The request body should be a JSON object with the following fields:\n","    * `name`: Your full name\n","    * `mail`: Your email address\n","    * `github_url`: The URL of your GitHub repository containing the solution\n","\n","**Deadline:**\n","\n","* The deadline for submitting the challenge is 5 calendar days after receiving the challenge.\n","\n","**Technology and Techniques:**\n","\n","* You can use any technology or technique you prefer for data processing.\n","* We will value your knowledge of cloud platforms.\n","* If you use cloud platforms, follow the steps in your files WITHOUT adding access credentials to the different services.\n","\n","**Ranking Criteria:**\n","\n","* Challenges that are clearly organized, explanatory, modular, efficient, and creative will be ranked higher.\n","\n","**Assumptions and Documentation:**\n","\n","* Write down the assumptions you are making.\n","* Include the versions of the libraries you are using in the requirements.txt file.\n","* Do not delete what is already written in the requirements.txt file.\n","* For this challenge, we recommend that you clearly describe how each part of your exercise can be improved.\n","\n","**Data:**\n","\n","* You must use the data contained in the provided file.\n","* You can use the official Twitter documentation to understand the data structure.\n","\n","**Git Usage:**\n","\n","* We will positively evaluate good practices of Git usage.\n","* Use the main branch for any final version you want us to review.\n","* We recommend that you use some GitFlow practice.\n","* Do not delete your development branches.\n","\n","**Error Handling and Edge Cases:**\n","\n","* Consider error handling and edge cases.\n","\n","**Maintainability, Readability, and Scalability:**\n","\n","* Remember that you will be working with other developers, so the maintainability, readability, and scalability of your code is essential.\n","\n","**Code Documentation:**\n","\n","* Good code documentation always helps the reader.\n","\n","**Additional Notes:**\n","\n","* The `README.md` file mentions using GitHub Desktop, Git Flow, and Colab. These are tools that can be used for version control and code collaboration.\n","* The `README.md` file also mentions measuring time and memory. This can be done using Python's built-in `time` and `memory_profiler` modules."],"metadata":{"id":"e0ytAOoEE50a"}},{"cell_type":"code","source":["import sys\n","\n","if __name__ != \"__main__\":\n","    sys.exit()\n","\n","# General libraries\n","import logging\n","import os\n","import time\n","\n","# Data related (if used later)\n","from typing import List, Tuple, Any, Optional\n","import datetime"],"metadata":{"id":"R45EuZ1JWSa4","executionInfo":{"status":"ok","timestamp":1712380488384,"user_tz":180,"elapsed":3,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## **Definitions and Configurations:**\n","The following code snippet defines constants used in the data transfer and processing pipeline:\n","\n","- **Google Cloud Storage (GCS) Information:**\n","    - `BUCKET_NAME`: Specifies the name of the GCS bucket where data will be uploaded (`tw-gcp-public-lab`).\n","    - `FOLDER_NAME`: Denotes the folder within the bucket to store the uploaded file (`raw`).\n","    - `ZIP_FILE_NAME`: Represents the name of the compressed file containing tweets data (`tweets.json.zip`).\n","    - `GCS_SOURCE_URI`: Constructs the full URI for the file location in GCS after upload (`gs://tw-gcp-public-lab/raw/`).\n","\n","- **Local File Paths:**\n","    - `SOURCE_PATH`: Currently defines a local file path (`/content/drive/Othercomputers/My Mac/latam-challenge`), but it's not used in the provided code for downloading.\n","\n","- **Google Cloud Project and Dataset Information:**\n","    - `PROJECT_ID`: Specifies the Google Cloud project ID (`tw-techdash`).\n","    - `DATASET_NAME`: Defines the name of the BigQuery dataset where the data will be loaded (`tweets_dataset`).\n","    - `TABLE_NAME`: Identifies the name of the BigQuery table to store the extracted tweets data (`tweets`).\n","\n","**Observations:**\n","\n","- The `SOURCE_PATH` might require modification if you intend to download a file from a different location.\n","- Consider using environment variables or a configuration file to manage these constants, making your code more flexible and easier to maintain."],"metadata":{"id":"hfr4Y-Cv8Cd9"}},{"cell_type":"code","source":["# Definitions\n","\n","# Notebook time measure\n","START_TIME:str = os.environ.get(\"START_TIME\", str(time.time()))\n","END_TIME:str = START_TIME\n","\n","# Google Cloud Storage (GCS) information\n","BUCKET_NAME: str = os.environ.get(\"BUCKET_NAME\", \"tw-gcp-public-lab\")\n","FOLDER_NAME: str = os.environ.get(\"FOLDER_NAME\", \"raw\")\n","ZIP_FILE_NAME: str = os.environ.get(\"ZIP_FILE_NAME\", \"tweets.json.zip\")\n","FILE_ID: str = os.environ.get(\"FILE_ID\", \"1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis\")\n","GCS_SOURCE_URI: str = os.environ.get(\"GCS_SOURCE_URI\", f\"gs://{BUCKET_NAME}/{FOLDER_NAME}/\")\n","\n","# Local file paths (consider user input/environment variables)\n","MOUNT_POINT: str = os.environ.get(\"MOUNT_POINT\", \"/content/drive\")\n","SOURCE_PATH: str = os.environ.get(\"SOURCE_PATH\", \"/content/drive/Othercomputers/My Mac/latam-challenge\")\n","\n","# Google Cloud project and dataset information (consider environment variables)\n","PROJECT_ID: str = os.environ.get(\"PROJECT_ID\", \"tw-techdash\")\n","DATASET_NAME: str = os.environ.get(\"DATASET_NAME\", \"tweets_dataset\")\n","TABLE_NAME: str = os.environ.get(\"TABLE_NAME\", \"tweets\")\n","\n","# Logging\n","LOGGING_LEVEL: str = os.environ.get(\"LOGGING_LEVEL\", str(logging.DEBUG))\n","LOGGING_FILE: str = os.environ.get(\"LOGGING_FILE\", f\"{SOURCE_PATH}/notebook.log\")\n","\n","# Configurations\n","# Logging\n","logging.basicConfig(filename=LOGGING_FILE, level=int(LOGGING_LEVEL))"],"metadata":{"id":"g57ic23AMKFp","executionInfo":{"status":"ok","timestamp":1712380488384,"user_tz":180,"elapsed":2,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## **Jupyter Kernel code reloading**\n","\n","**Functionality:**\n","\n","* This code snippet utilizes magic commands within Jupyter Notebooks to manage code reloading.\n","* The `%reload_ext autoreload` line imports and activates the `autoreload` extension.\n","* The `%autoreload 2` line configures the `autoreload` extension to automatically reload Python modules when changes are detected.\n","\n","**Key Concepts:**\n","\n","* **Jupyter Magic Commands:** `%` prefix is used for magic commands that provide special functionality within Jupyter notebooks.\n","* **Autoreload Extension:**  A Jupyter extension that automatically reloads Python modules when changes are detected in the corresponding source files.\n","* **Reload Level:** The level `2` specifies that reload should occur when source files or any imported modules are modified (level 1 only reloads source file changes).\n","\n","**Overall Assessment:**\n","\n","* This code improves development efficiency within Jupyter notebooks by automatically reloading code, avoiding manual restarts.\n","* It leverages the `autoreload` extension for automatic reloading functionality.\n","* The configuration level `2` ensures comprehensive reloading behavior.\n","\n","**Potential Enhancements:**\n","\n","* While automatic reloading is helpful in development, it might not be suitable for production environments due to potential unexpected behavior during execution.\n","* Consider using this approach primarily for interactive development within Jupyter notebooks."],"metadata":{"id":"n0zd65ksIbea"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"fkniVfT9W5hr","executionInfo":{"status":"ok","timestamp":1712380488384,"user_tz":180,"elapsed":2,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}}},"outputs":[],"source":["# Enable automatic reloading of modules in Jupyter Notebook (improves development workflow)\n","%reload_ext autoreload\n","\n","# Automatically restart the kernel whenever the source code changes\n","# (Provides a clean development environment)\n","%autoreload 2"]},{"cell_type":"markdown","source":["## **Google Drive mounting**\n","\n","**Functionality:**\n","\n","1. **Connects Google Drive:** This code establishes a connection between your Google Drive storage and the virtual machine running the Colab notebook.\n","2. **Navigates to Project Directory:** This magic command changes the working directory within the Colab notebook to a specific location within your project directory.\n","\n","**Key Concepts:**\n","\n","* **Google Drive Mounting:**\n","    - `from google.colab import drive`: Imports the `drive` module for interacting with Google Drive from Colab.\n","    - `drive.mount('/content/drive', force_remount=True)`: Mounts your Drive at the `/content/drive` path within Colab.\n","    - **Authorization:** Requires initial authorization to grant Colab access to your Drive.\n","* **Jupyter Notebook Magic Commands:**\n","    - `%cd`: A magic command specifically designed for changing directories.\n","\n","**Overall Assessment:**\n","\n","* **Convenient Data Access:** Enables seamless access to your personal data stored in Google Drive for use within Colab notebooks.\n","* **Improved Code Organization:** Helps organize your notebook within the project structure by focusing on a specific subdirectory (like \"src\").\n","\n","**Potential Enhancements:**\n","\n","* **Google Drive Mounting:**\n","    - **Error Handling:** Consider incorporating `try-except` blocks to gracefully handle potential mounting issues.\n","    - **Authentication Persistence:** Explore ways to persist the authentication token (if applicable) to avoid re-authorization for every session.\n","* **Navigation:**\n","    - **Clear Path Definitions:** Replace `{SOURCE_PATH}` with the actual path to your project directory for clarity.\n","    - **Error Handling:** Consider handling potential issues like non-existent directories using Python code (like `try-except` blocks).\n","\n","**Explanation:**\n","\n","1. **Mount Google Drive:** The first part of the code imports the `drive` module and mounts your Google Drive to the `/content/drive` directory within Colab. This allows you to access your Drive files from within your notebook.\n","2. **Change Directory:** The `%cd {SOURCE_PATH}/src` line uses a magic command to navigate to the subdirectory named \"src\" within your project directory (assuming `{SOURCE_PATH}` points to the correct location). This helps organize your notebook by focusing on the relevant project code.\n","\n","**Important Notes:**\n","\n","* Replace `{SOURCE_PATH}` with the actual path to your project directory on your machine.\n","* You'll need to go through an authorization process the first time you run the mounting code to grant Colab access to your Drive."],"metadata":{"id":"KcY78DdKJ1V_"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","def mount_google_drive(\n","    mount_point: str = '/content/drive'\n",") -> None:\n","  \"\"\"Mounts Google Drive to the specified mount point.\n","\n","  Args:\n","      mount_point (str, optional): The path to mount Google Drive. Defaults to '/content/drive'.\n","\n","  Raises:\n","      RuntimeError: If there's an error mounting the drive.\n","  \"\"\"\n","  logging.info(f\"Attempting to mount Google Drive to {mount_point}\")\n","  try:\n","    drive.mount(mount_point, force_remount=True)\n","    logging.info(f\"Successfully mounted Google Drive to {mount_point}\")\n","  except Exception as e:\n","    logging.error(f\"Error mounting Google Drive: {e}\")\n","    raise RuntimeError(f\"Error mounting Google Drive: {e}\")\n","\n","# Mount Google Drive (optional: call the function)\n","mount_google_drive()\n","\n","# Change directory using a more explicit method\n","logging.info(f\"Changing directory to: {os.path.join(MOUNT_POINT, SOURCE_PATH, 'src')}\")\n","os.chdir(os.path.join(MOUNT_POINT, SOURCE_PATH, 'src'))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IV-79dMCpYAh","executionInfo":{"status":"ok","timestamp":1712380491959,"user_tz":180,"elapsed":3577,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"2a983412-e7a3-42f5-a94b-197ecf1be7b1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## **Libraries requirements**\n","\n","**Functionality:**\n","\n","- **Installs Python Libraries:** This code snippet installs a collection of Python libraries listed in a file named `requirements.txt` within the currently active virtual environment.\n","\n","**Key Concepts:**\n","\n","- **requirements.txt File:** This text file contains a list of library names and their version requirements, ensuring consistent installation across environments.\n","- **Virtual Environments:** Virtual environments isolate project dependencies, preventing conflicts with other Python projects on your system.\n","- **sys.executable:** This Python variable points to the path of the Python interpreter for the active virtual environment.\n","- **pip:** The Python Package Installer (pip) is used for managing Python packages and libraries.\n","\n","**Explanation:**\n","\n","1. **`import sys`**: Imports the `sys` module, providing access to system-specific variables and functions.\n","2. **`!{sys.executable} -m pip install -r '../requirements.txt'`**: This line calls the pip installer within the virtual environment:\n","   - **`!`**: Jupyter Notebook magic command to execute terminal commands.\n","   - **`{sys.executable}`**: Ensures pip is called from the virtual environment's Python interpreter.\n","   - **`-m`**: Designates a module to execute as a script (in this case, `pip`).\n","   - **`install -r`**: Instructs pip to install packages from a requirements file.\n","   - **`'../requirements.txt'`**: Specifies the path to the requirements file (relative to the current notebook's directory).\n","\n","**Important Notes:**\n","\n","- **Virtual Environment Activation:** Ensure you've activated the desired virtual environment before running this code.\n","- **Path to requirements.txt:** Verify that `../requirements.txt` correctly points to the file's location.\n","- **Internet Connection:** An internet connection is required for pip to download and install packages.\n","\n","**Overall Assessment:**\n","\n","- **Efficient Dependency Management:** Using `requirements.txt` is a best practice for managing project dependencies consistently.\n","- **Consistent Environments:** Facilitates consistent library installations across different machines for reproducibility.\n","- **Collaboration:** Enables easy setup of the same project environment for others.\n","\n","**Potential Enhancements:**\n","\n","- **Error Handling:** Consider incorporating error handling (like try-except blocks) to gracefully handle potential issues during installation, such as network connectivity problems or missing packages.\n"],"metadata":{"id":"KuUJyVVpKQjJ"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"V_d7gErJW5hs","executionInfo":{"status":"ok","timestamp":1712380509279,"user_tz":180,"elapsed":17321,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"698ad37b-f8c4-4cb4-a22b-3dc0afd4d9bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully installed libraries from requirements.txt\n"]}],"source":["def install_requirements(\n","    requirements_path: str = \"../requirements.txt\"\n",") -> None:\n","    \"\"\"Installs Python libraries from the specified requirements file.\n","\n","    Args:\n","        requirements_path (str, optional): Path to the requirements.txt file. Defaults to \"../requirements.txt\".\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    import subprocess\n","\n","    try:\n","        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", requirements_path], check=True)\n","        print(\"Successfully installed libraries from requirements.txt\")\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Error installing libraries: {e}\")\n","\n","# Call the install function\n","install_requirements()\n"]},{"cell_type":"markdown","source":["## **Ingest Google Drive ZIP into Google Storage**\n","**Key Functions:**\n","\n","1. **authenticate_google_drive()**: Authenticates with Google Drive using the user's credentials.\n","2. **download_file_from_drive(drive_service, file_id)**: Downloads a specified file from Google Drive.\n","3. **upload_file_to_cloud_storage(client, bucket_name, folder_name, downloaded, zip_file_name)**: Uploads a file to Google Cloud Storage, handling folder creation if needed.\n","4. **decompress_zip_file(client, bucket_name, folder_name, zip_file_name)**: Decompresses a ZIP file within a GCS bucket.\n","\n","**Code Structure:**\n","\n","- **Logging:** Employs `logging` for debugging and tracking progress.\n","- **Error Handling:** Uses try-except blocks to gracefully handle potential errors.\n","- **Modularity:** Separates functionality into distinct, reusable functions.\n","- **Type Hints:** Enhances code readability and potential type checking.\n","\n","**Main Code Execution:**\n","\n","1. Configures logging to a file named 'transfer.log'.\n","2. Authenticates with Google Drive.\n","3. Downloads the specified file from Drive.\n","4. Creates a Cloud Storage client.\n","5. Uploads the downloaded file to GCS.\n","6. Decompresses the ZIP file in GCS if its content type is 'application/zip'.\n","7. Logs success or failure messages.\n","8. Finally, ensures the downloaded file is closed.\n","\n","**Overall Assessment:**\n","\n","- **Well-structured:** The code is organized, modular, and includes error handling.\n","- **Clear Functionality:** It effectively handles file transfer and decompression tasks.\n","- **Authentication Flexibility:** Uses authentication methods external to the code (useful for avoiding credentials in code).\n","- **Good Practices:** Adheres to good practices like logging and try-except blocks.\n","\n","**Potential Enhancements:**\n","\n","- **Parameterization:** Explore using command-line arguments or configuration files to adjust parameters more flexibly.\n","- **Progress Reporting:** Consider more granular progress reporting for downloads/uploads.\n","- **Content Validation:** Validate file content after decompression for integrity.\n","- **Advanced Error Handling:** Implement retries or alternative actions for potential errors.\n","\n","This code provides a foundation for file transfer and decompression tasks within Google Cloud environments, demonstrating clarity and attention to best practices."],"metadata":{"id":"PA2wPhkO7khO"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"pz4EZrB4W5hu","outputId":"d89d76d9-ebcd-4104-fd82-776f9e9bd278","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712380528334,"user_tz":180,"elapsed":19057,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading 100%\n","File uploaded to gs://tw-gcp-public-lab/raw/tweets.json.zip\n","File decompressed in gs://tw-gcp-public-lab/raw/farmers-protest-tweets-2021-2-4.json\n","File transfer process completed.\n"]}],"source":["from google.colab import auth\n","from googleapiclient.discovery import build\n","import io\n","from googleapiclient.http import MediaIoBaseDownload\n","from google.cloud import storage\n","import zipfile\n","\n","\n","def authenticate_google_drive() -> None:\n","    \"\"\"Authenticates to Google Drive using the user's credentials.\n","\n","    Args:\n","        None\n","    \"\"\"\n","    try:\n","        auth.authenticate_user()\n","    except Exception as e:\n","        logging.error(f\"Error authenticating to Google Drive: {e}\")\n","        raise\n","\n","\n","def download_file_from_drive(\n","    drive_service: Any, file_id: str\n",") -> io.BytesIO:\n","    \"\"\"Downloads a file from Google Drive.\n","\n","    Args:\n","        drive_service (Any): The Google Drive service resource (in the build function returns Any).\n","        file_id (str): The ID of the file to download.\n","\n","    Returns:\n","        io.BytesIO: The downloaded file content as a BytesIO object.\n","    \"\"\"\n","    downloaded: io.BytesIO = io.BytesIO()\n","    try:\n","        request = drive_service.files().get_media(fileId=file_id)\n","        downloader = MediaIoBaseDownload(downloaded, request)\n","        done = False\n","        while not done:\n","            status, done = downloader.next_chunk()\n","            print(f'Downloading {int(status.progress() * 100)}%')\n","        downloaded.seek(0)\n","        return downloaded\n","    except Exception as e:\n","        print(f\"Error downloading file: {e}\")\n","        raise\n","\n","\n","def upload_file_to_cloud_storage(\n","    bucket: storage.Bucket, folder_name: str, downloaded: io.BytesIO, zip_file_name: str\n",") -> storage.Blob:\n","    \"\"\"Uploads a file to Google Cloud Storage.\n","\n","    Args:\n","        bucket (Bucket): Google Cloud Storage bucket.\n","        folder_name (str): The name of the folder within the bucket where the file will be uploaded.\n","        downloaded (io.BytesIO): The downloaded file to upload.\n","        zip_file_name (str): The name of the file to be uploaded.\n","\n","    Returns:\n","        google.cloud.storage.Blob: The uploaded blob object.\n","    \"\"\"\n","    folder_blob: storage.Blob = bucket.blob(f\"{folder_name}/\")\n","\n","    # Check and create folder if it doesn't exist\n","    if not folder_blob.exists():\n","        folder_blob.upload_from_string('', content_type='application/x-www-form-urlencoded;charset=UTF-8')\n","\n","    # Upload the file to the specified folder\n","    blob: storage.Blob = bucket.blob(f'{folder_name}/{zip_file_name}')\n","    blob.upload_from_file(downloaded, content_type='application/zip')\n","\n","    print(f'File uploaded to gs://{bucket.name}/{blob.name}')\n","    return blob\n","\n","\n","def decompress_zip_file(\n","    bucket: storage.Bucket, folder_name: str, zip_file_name: str\n",") -> str:\n","    \"\"\"Decompresses a ZIP file stored in Google Cloud Storage.\n","\n","    Args:\n","        bucket (Bucket): Google Cloud Storage bucket where the ZIP file is stored.\n","        folder_name (str): The name of the folder within the bucket where the ZIP file is located.\n","        zip_file_name (str): The name of the ZIP file\n","\n","    Returns:\n","        str: The unzipped file name.\n","    \"\"\"\n","\n","    json_file_name: str = ''\n","    blob_name: str = ''\n","\n","    try:\n","        zip_blob: storage.Blob = bucket.blob(f'{folder_name}/{zip_file_name}')\n","        with zipfile.ZipFile(io.BytesIO(zip_blob.download_as_string()), 'r') as z:\n","            for file_info in z.infolist():\n","                with z.open(file_info) as file:\n","                    blob_name: str = f'{folder_name}/{file_info.filename}'\n","                    json_file_name: str = file_info.filename\n","                    json_blob: storage.Blob = bucket.blob(blob_name)\n","                    json_blob.upload_from_file(file)\n","        print(f'File decompressed in gs://{bucket.name}/{blob_name}')\n","    except zipfile.BadZipFile:\n","        logging.warning(f'The file in gs://{bucket.name}/{folder_name}/{zip_file_name} is not a valid ZIP file.')\n","    except Exception as e:\n","        logging.error(f'Error decompressing file: {e}')\n","    finally:\n","        return json_file_name\n","\n","downloaded: io.BytesIO = io.BytesIO()\n","\n","try:\n","    # Authenticate to Google Drive\n","    authenticate_google_drive()\n","    drive_service: Any = build('drive', 'v3')\n","\n","    # Download file from Drive\n","    downloaded: io.BytesIO = download_file_from_drive(drive_service, FILE_ID)\n","\n","    # Create Cloud Storage Bucket\n","    bucket: storage.Bucket = storage.Client().bucket(BUCKET_NAME)\n","\n","    # Upload file to Cloud Storage\n","    uploaded_blob: storage.Blob = upload_file_to_cloud_storage(bucket, FOLDER_NAME, downloaded, ZIP_FILE_NAME)\n","\n","    # Decompress ZIP file if applicable\n","    if uploaded_blob.content_type == 'application/zip':\n","        json_file_name: str = decompress_zip_file(bucket, FOLDER_NAME, ZIP_FILE_NAME)\n","\n","    logging.info(\"File transfer successful!\")\n","\n","except Exception as e:\n","    logging.error(f\"An error occurred: {e}\")\n","\n","finally:\n","    downloaded.close()  # Close downloaded file\n","    print(\"File transfer process completed.\")\n"]},{"cell_type":"markdown","source":["## **BigQuery Storage Functions**\n","\n","**Functionality:**\n","\n","These Python functions interact with BigQuery to authenticate, create datasets and tables, and load data from Cloud Storage.\n","\n","**Key Concepts:**\n","\n","* **Client:** The `bigquery.Client` object is central to interacting with BigQuery.\n","* **Datasets and Tables:** Datasets organize tables, and both can be created or overwritten using these functions.\n","* **Data Loading:** Data is loaded from Cloud Storage in newline-delimited JSON format, and BigQuery automatically infers the schema.\n","* **Error Handling:** The functions use logging and try-except blocks to handle errors and provide informative messages.\n","\n","**Snippet 1: authenticate_bigquery**\n","\n","**Functionality:**\n","\n","Authenticates to BigQuery and returns a client object for subsequent operations.\n","\n","**Key Concepts:**\n","\n","* **Project ID:** Required for authentication.\n","\n","**Overall Assessment:**\n","\n","Clear and concise function for initial setup.\n","\n","**Potential Enhancements:**\n","\n","* **Error Handling:** Consider logging errors with more detail.\n","\n","**Snippet 2: create_dataset**\n","\n","**Functionality:**\n","\n","Creates a dataset if it doesn't exist or overwrites it if specified.\n","\n","**Key Concepts:**\n","\n","* **Mode:** Optional argument to control actions if the dataset already exists.\n","\n","**Overall Assessment:**\n","\n","Good flexibility with `mode` argument for handling existing datasets.\n","\n","**Potential Enhancements:**\n","\n","* **Input Validation:** Consider validating dataset names for compliance with BigQuery rules.\n","\n","**Snippet 3: create_table**\n","\n","**Functionality:**\n","\n","Creates a table within a dataset if it doesn't exist or overwrites it if specified.\n","\n","**Key Concepts:**\n","\n","* **Schema Inference:** Uses an empty schema to let BigQuery infer it from the data.\n","\n","**Overall Assessment:**\n","\n","Handles table creation effectively.\n","\n","**Potential Enhancements:**\n","\n","* **Schema Definition:** Explore allowing optional schema definition for more control.\n","\n","**Snippet 4: load_data_from_storage**\n","\n","**Functionality:**\n","\n","Loads data from a newline-delimited JSON file in Cloud Storage to a BigQuery table.\n","\n","**Key Concepts:**\n","\n","* **Load Job Configuration:** Specifies data format, schema inference, and handling of unknown values.\n","\n","**Overall Assessment:**\n","\n","Well-structured data loading process.\n","\n","**Potential Enhancements:**\n","\n","* **Progress Reporting:** Consider logging loading progress.\n","* **Data Validation:** Explore adding data validation checks before loading.\n"],"metadata":{"id":"Dh8NblbcCU8H"}},{"cell_type":"code","source":["from google.cloud import bigquery\n","from google.api_core.exceptions import NotFound\n","\n","\n","def authenticate_bigquery(\n","    project_id: str\n",") -> bigquery.Client:\n","    \"\"\"Authenticates to BigQuery and returns the client object.\n","\n","    Args:\n","        project_id (str): Your GCP project ID.\n","\n","    Returns:\n","        bigquery.Client: BigQuery client object.\n","    \"\"\"\n","\n","    return bigquery.Client(project_id)\n","\n","\n","def create_dataset(\n","    client: bigquery.Client, dataset_name: str, mode: str = 'create'\n",") -> None:\n","    \"\"\"\n","    Creates a BigQuery dataset if it doesn't exist.\n","\n","    Args:\n","        client (bigquery.Client): BigQuery client object.\n","        dataset_name (str): Name of the dataset to create.\n","        mode (Optional[str], optional): Action to take if the dataset already exists ('create' or 'overwrite'). Defaults to 'create'.\n","\n","    Raises:\n","        Exception: For unexpected errors during dataset creation or existence check.\n","    \"\"\"\n","\n","    dataset_ref: bigquery.DatasetReference = client.dataset(dataset_name)\n","    try:\n","        client.get_dataset(dataset_ref)\n","        if mode == 'overwrite':\n","            logging.info(f\"Dataset '{dataset_name}' already exists, overwriting...\")\n","            client.delete_dataset(dataset_ref, delete_contents=True)\n","            client.create_dataset(dataset_ref)\n","            logging.info(f\"Dataset '{dataset_name}' overwritten.\")\n","        else:\n","            logging.info(f\"Dataset '{dataset_name}' already exists.\")\n","    except NotFound:\n","        logging.info(f\"Dataset '{dataset_name}' not found, creating...\")\n","        client.create_dataset(dataset_ref)\n","        logging.info(f\"Dataset '{dataset_name}' created.\")\n","    except Exception as e:\n","        logging.error(f\"Error creating dataset '{dataset_name}': {e}\")\n","        raise\n","\n","\n","def create_table(\n","    client: bigquery.Client, dataset_name: str, table_name: str, mode: str = 'create'\n",") -> None:\n","    \"\"\"\n","    Creates a BigQuery table if it doesn't exist.\n","\n","    Args:\n","        client (bigquery.Client): BigQuery client object.\n","        dataset_name (str): Name of the dataset containing the table.\n","        table_name (str): Name of the table to create.\n","        mode (Optional[str], optional): Action to take if the table already exists ('create' or 'overwrite'). Defaults to 'create'.\n","\n","    Raises:\n","        Exception: For unexpected errors during table creation or existence check.\n","    \"\"\"\n","\n","    dataset_ref: bigquery.DatasetReference = client.dataset(dataset_name)\n","    table_ref: bigquery.TableReference = dataset_ref.table(table_name)\n","    try:\n","        client.get_table(table_ref)\n","        logging.info(f\"Table '{table_name}' already exists.\")\n","        if mode == 'overwrite':\n","            logging.info(f\"Overwriting table '{table_name}'...\")\n","            client.delete_table(table_ref)\n","            table: bigquery.Table = bigquery.Table(table_ref)\n","            client.create_table(table) # Empty schema for BigQuery to infer\n","            logging.info(f\"Table '{table_name}' overwritten.\")\n","    except NotFound:\n","        logging.info(f\"Table '{table_name}' not found, creating...\")\n","        table: bigquery.Table = bigquery.Table(table_ref)\n","        client.create_table(table) # Empty schema for BigQuery to infer\n","        logging.info(f\"Table '{table_name}' created.\")\n","    except Exception as e:\n","        logging.error(f\"Error creating table '{table_name}': {e}\")\n","        raise\n","\n","def load_data_from_storage(\n","    client: bigquery.Client, source_uri: str, dataset_name: str, table_name: str\n",") -> None:\n","    \"\"\"\n","    Loads data from Cloud Storage (newline-delimited JSON) to BigQuery table.\n","\n","    Args:\n","        client (bigquery.Client): BigQuery client object.\n","        source_uri (str): URI of the data file in Cloud Storage.\n","        dataset_name (str): Name of the dataset containing the table.\n","        table_name (str): Name of the table to load data into.\n","\n","    Raises:\n","        Exception: For unexpected errors during data loading.\n","    \"\"\"\n","\n","    job_config: bigquery.LoadJobConfig = bigquery.LoadJobConfig()\n","\n","    # Only way to set job_config properties is without type notation\n","    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n","    job_config.autodetect = True  # Auto-detect schema\n","    job_config.ignore_unknown_values = True  # Ignore unknown values\n","\n","    load_job = client.load_table_from_uri(\n","        source_uri + json_file_name,\n","        client.dataset(dataset_name).table(table_name),\n","        job_config=job_config\n","    )\n","    try:\n","        load_job.result()  # Wait for load completion\n","        logging.info(f\"Data loaded from '{source_uri}' to table '{dataset_name}.{table_name}'.\")\n","    except Exception as e:\n","        logging.error(f\"Error loading data: {e}\")\n","        raise\n","\n","\n","# Authenticate to BigQuery (assuming PROJECT_ID is defined elsewhere)\n","bigquery_client: bigquery.Client = authenticate_bigquery(PROJECT_ID)\n","\n","# Create dataset (overwrite if needed)\n","create_dataset(bigquery_client, DATASET_NAME, mode='overwrite')\n","\n","# Create table (overwrite if needed)\n","create_table(bigquery_client, DATASET_NAME, TABLE_NAME, mode='overwrite')\n","\n","# Load data from Cloud Storage\n","load_data_from_storage(bigquery_client, GCS_SOURCE_URI, DATASET_NAME, TABLE_NAME)\n","\n","print(\"Data loading completed!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"teRz4l7jW-fq","executionInfo":{"status":"ok","timestamp":1712380553100,"user_tz":180,"elapsed":24778,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"19b759fb-e1f6-41b9-a008-251a191637e3"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Data loading completed!\n"]}]},{"cell_type":"markdown","source":["## **BigQuery Processing Functions**\n","\n","**Functionality:**\n","\n","- **Processes BigQuery Results:** The `process_bigquery_results` function executes the query, handles results, and converts them into a desired format (list of tuples with date and username).\n","\n","**Key Concepts:**\n","\n","- **Type Hints:** Employs type hints (`List`, `Tuple`, `datetime.date`) for improved code readability and potential static type checking.\n","- **Error Handling:** Incorporates `try-except` blocks to gracefully handle exceptions (`BadRequest` and generic exceptions).\n","- **Data Conversion:** Converts retrieved data rows into the specified format.\n","\n","**Overall Assessment:**\n","\n","- **Clear Separation:** Functions promote modularity and reusability.\n","- **Meaningful Variable Names:** Descriptive names enhance code understandability.\n","- **Error Management:** Handles potential errors during query execution and processing.\n","\n","**Potential Enhancements:**\n","\n","- **Input Validation:** Consider validating the constructed query string before execution.\n","- **Logging:** Integrate logging for detailed tracking and debugging.\n","- **Security:** Ensure secure credential management for BigQuery access.\n","- **Query Parameterization:** If DATASET_NAME and TABLE_NAME are not intended for hardcoding, utilize BigQuery's query parameters for better reusability and security.\n","- **Data Usage:** Currently, the extracted data is printed. You can modify this section to store the data in a desired location or perform further processing.\n","\n","This code provides a foundation for working with BigQuery data retrieval and processing. You can extend it based on your specific needs."],"metadata":{"id":"dEcJqRweD26B"}},{"cell_type":"code","source":["from google.api_core.exceptions import BadRequest, NotFound  # Specific exceptions\n","\n","def process_bigquery_results(\n","    client: bigquery.Client, query: str\n",") -> List[Tuple[Any, Any]]:\n","    \"\"\"\n","    Executes a BigQuery query, handles results, and performs data conversion.\n","\n","    Args:\n","        client: BigQuery client object.\n","        query: BigQuery SQL query string.\n","\n","    Returns:\n","        A list of tuples containing the extracted data (date and username).\n","\n","    Raises:\n","        NotFound: If the query returns no results.\n","        Exception: For other unexpected errors during query execution or processing.\n","    \"\"\"\n","\n","    extracted_data: List[Tuple[str, str]] = []\n","\n","    try:\n","        query_job: bigquery.QueryJob = client.query(query)\n","        results = query_job.result() # Type notation not possible for this\n","\n","        if not results:\n","            raise NotFound(\"No results found for the query.\")\n","\n","        extracted_data = [(row[0], row[1]) for row in results]\n","\n","    except BadRequest as e:\n","        print(f\"BigQuery error: {e}\")\n","        raise\n","    except NotFound as e:\n","        print(f\"Query returned no results: {e}\")\n","    except Exception as e:\n","        print(f\"Error: {e}\")\n","        raise\n","    finally:\n","        return extracted_data\n"],"metadata":{"id":"WLlwchwVZqYx","executionInfo":{"status":"ok","timestamp":1712380553100,"user_tz":180,"elapsed":7,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# TODO: check this\n","#import q1_time\n","\n","# Llama a la función q1_time para obtener el resultado\n","#resultado = q1_time.q1_time(file_path)\n","\n","# Imprime el resultado\n","#print(resultado)"],"metadata":{"id":"5oTEJKuIMCvP","executionInfo":{"status":"ok","timestamp":1712380553100,"user_tz":180,"elapsed":6,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## **BigQuery Queries**\n","\n","**Snippet 1: Top 10 Dates with Top Users**\n","\n","**Functionality**\n","\n","This SQL query identifies the top 10 dates with the most tweets and, for each of those dates, finds the user with the most tweets (considering usernames alphabetically in case of ties).\n","\n","**Key Concepts**\n","\n","* **Common Table Expressions (CTEs):** The query utilizes two CTEs:\n","    * `TopDates`: Calculates the daily tweet count and ranks them in descending order, selecting the top 10.\n","    * `TopUsersDate`: Joins the `tweets` table with `TopDates` to find the user(s) with the most tweets for each top date. It uses `ROW_NUMBER()` to handle ties by username order.\n","* **Window Functions:** `ROW_NUMBER()` is used within `TopUsersDate` to assign a unique row number within each date partition, ordered by tweet count (descending) and then by number of tweets per user (descending).\n","* **Filtering:** The final result retrieves users with `row_number = 1` (the user with the most tweets for each date).\n","\n","**Overall Assessment**\n","\n","This query effectively addresses the task by leveraging CTEs for modularity and window functions to handle ranking and ties.\n","\n","**Potential Enhancements**\n","\n","* **Clarity:** Consider adding comments within the query to explain the purpose of each CTE.\n","* **Efficiency:** Explore alternative approaches to handle ties if performance is critical.\n","\n","**Data Usage**\n","\n","The query currently prints the `tweets_date` and `username`. You might want to consider storing this information in a table or using it for further analysis.\n","\n","**Snippet 2: Top 10 Most Used Emojis**\n","\n","**Functionality**\n","\n","This query extracts emojis from tweets and identifies the top 10 most frequently used emojis along with their counts.\n","\n","**Key Concepts**\n","\n","* **Regular Expressions (RegEx):** The `REGEXP_EXTRACT_ALL()` function utilizes a complex RegEx pattern to capture a wide range of emoji characters across different Unicode blocks.\n","* **UNNEST:** The `UNNEST()` operator is used to explode the extracted emoji list into a single row per emoji for counting.\n","\n","**Overall Assessment**\n","\n","This query effectively extracts and counts emojis, providing valuable insights into emoji usage.\n","\n","**Potential Enhancements**\n","\n","* **Filtering:** Depending on the analysis goals, you might want to filter out specific emoji categories (e.g., flags, country codes).\n","* **Normalization:** Consider normalizing emojis to a canonical form to handle variations (e.g., skin tone modifiers).\n","\n","**Data Usage**\n","\n","The query currently prints the `emoji` and `count`. You could store this information for further analysis of emoji popularity.\n","\n","**Snippet 3: Top 10 Influential Users**\n","\n","**Functionality**\n","\n","This query identifies the top 10 users with the most mentions (`@username`) received in tweets.\n","\n","**Key Concepts**\n","\n","* **UNNEST:** Similar to snippet 2, `UNNEST()` is used to explode the mentioned user list from each tweet for counting mentions.\n","\n","**Overall Assessment**\n","\n","This query effectively identifies influential users based on mentions.\n","\n","**Potential Enhancements**\n","\n","* **Filtering:** You might consider filtering out self-mentions or mentions from specific accounts.\n","* **Weighted Mentions:** Depending on the analysis goals, explore assigning weights to mentions based on factors like follower count.\n","\n","**Data Usage**\n","\n","The query currently prints the `username` and `mention_count`. You could store this information for further analysis of user influence."],"metadata":{"id":"5xm-D9ctF0Nx"}},{"cell_type":"markdown","source":[],"metadata":{"id":"kZOeNPrCbbU5"}},{"cell_type":"code","source":["import queries # For src py files it shows import unresolved but it works anyways\n","\n","queries.top_dates_with_top_users"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"kKjm82w1ES1F","executionInfo":{"status":"ok","timestamp":1712380553100,"user_tz":180,"elapsed":6,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"9aa9c7e8-88aa-4333-b470-146e1667f330"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n    WITH \\n    TopDates AS (\\n        SELECT\\n            CAST(date AS DATE) AS tweets_date,\\n            COUNT(id) AS tweet_count\\n        FROM tweets_dataset.tweets\\n        WHERE id IS NOT NULL\\n        GROUP BY tweets_date\\n        ORDER BY tweet_count DESC\\n        LIMIT 10\\n    ),\\n    TopUsersDate AS (\\n        SELECT\\n            TD.tweets_date,\\n            TW.user.username,\\n            MAX(TD.tweet_count) AS max_tweet_count,\\n            COUNT(TW.id) AS user_tweet_count,\\n            ROW_NUMBER() OVER (\\n                PARTITION BY TD.tweets_date \\n                ORDER BY MAX(TD.tweet_count) DESC, COUNT(*) DESC\\n            ) AS row_number\\n        FROM tweets_dataset.tweets AS TW\\n        INNER JOIN TopDates AS TD\\n            ON TD.tweets_date = CAST(TW.date AS DATE)\\n        WHERE TW.id IS NOT NULL\\n        GROUP BY\\n            TD.tweets_date,\\n            TW.user.username\\n        ORDER BY\\n            max_tweet_count DESC,\\n            user_tweet_count DESC,\\n            TW.user.username ASC\\n    )\\n\\n    SELECT\\n        tweets_date,\\n        username\\n    FROM TopUsersDate\\n    WHERE row_number = 1\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["process_bigquery_results(bigquery_client, queries.top_dates_with_top_users)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KlRWWiZoYhcU","executionInfo":{"status":"ok","timestamp":1712380554801,"user_tz":180,"elapsed":1706,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"99daa57a-03af-45d5-b639-0ae577cce167"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n"," (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n"," (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n"," (datetime.date(2021, 2, 16), 'jot__b'),\n"," (datetime.date(2021, 2, 14), 'rebelpacifist'),\n"," (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n"," (datetime.date(2021, 2, 15), 'jot__b'),\n"," (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n"," (datetime.date(2021, 2, 23), 'Surrypuria'),\n"," (datetime.date(2021, 2, 19), 'Preetm91')]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":[],"metadata":{"id":"hCtOM6dZYhgq","executionInfo":{"status":"ok","timestamp":1712380554801,"user_tz":180,"elapsed":6,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def hola():\n","    print(\"¡Hola desde el notebook!\")"],"metadata":{"id":"PpNZUn_HNh2r","executionInfo":{"status":"ok","timestamp":1712380554801,"user_tz":180,"elapsed":6,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["%store hola"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lff4MhkSNh6M","executionInfo":{"status":"ok","timestamp":1712380554801,"user_tz":180,"elapsed":6,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"11c0fdf4-2e0f-417c-88fd-3de8f25cdd7b"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Warning:hola is <function hola at 0x7a91f8a6c820>\n","Proper storage of interactively declared classes (or instances\n","of those classes) is not possible! Only instances\n","of classes in real modules on file system can be %store'd.\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ClKSN3x6Nh8u","executionInfo":{"status":"ok","timestamp":1712380554801,"user_tz":180,"elapsed":5,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import measure\n","\n","# Measure elapsed time\n","print_elapsed_time(measure_elapsed_time(START_TIME))  # Print the result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305},"id":"Kb9SWakoNiJe","executionInfo":{"status":"error","timestamp":1712380697407,"user_tz":180,"elapsed":3,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"5d062876-f4ef-43c6-c6d3-0e65bb39aa99"},"execution_count":18,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'time' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-0382003b6b26>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Measure elapsed time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeasure_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTART_TIME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Print the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/drive/Othercomputers/My Mac/latam-challenge/src/measure.py\u001b[0m in \u001b[0;36mmeasure_elapsed_time\u001b[0;34m(start_time)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \"\"\"\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0melapsed_time\u001b[0m  \u001b[0;31m# Return the elapsed time for further use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}