{"cells":[{"cell_type":"markdown","metadata":{"id":"jVGOHFL3W5hp"},"source":["## **LATAM Challenge - Data Ingest, Storage and Processing with Google Drive, Google Cloud Storage and Google BigQuery in Google Colab (Jupyter) with Python 3.10**\n","\n","**Welcome to the Data Engineer Challenge.** On this occasion, you will have the opportunity to get closer to the reality of the role, demonstrate your skills and knowledge in data processing with Python and different data structures.\n","\n","**Preparation:**\n","\n","1. Initial project commit (done with GitHub desktop)\n","2. Install Git Flow with `brew install git-flow`\n","3. Configure the repository with `git flow init`\n","4. Configure feature finishes to be done only in develop with `git config gitflow.feature.finish.keepremote true`\n","5. Synchronize the repository with GDrive\n","6. Read the code from GDrive with Colab\n","\n","**Additional Notes:**\n","\n","* The `README.md` file mentions using GitHub Desktop, Git Flow, and Colab. These are tools that can be used for version control and code collaboration.\n","* The `README.md` file also mentions measuring time and memory. This can be done using Python's built-in `time` and `memory_profiler` modules.\n","* English was used for both documentation and code."]},{"cell_type":"markdown","metadata":{"id":"e0ytAOoEE50a"},"source":["## **Challenge Guidelines:**\n","\n","**Repository:**\n","\n","* Your solution must be in a public repository on the GitHub platform.\n","\n","**Submitting your challenge:**\n","\n","1. Make a POST request to [https://advana-challenge-check-api-cr-k4hdbggvoq-uc.a.run.app/data-engineer](https://advana-challenge-check-api-cr-k4hdbggvoq-uc.a.run.app/data-engineer).\n","2. The request body should be a JSON object with the following fields:\n","    * `name`: Your full name\n","    * `mail`: Your email address\n","    * `github_url`: The URL of your GitHub repository containing the solution\n","\n","**Deadline:**\n","\n","* The deadline for submitting the challenge is 5 calendar days after receiving the challenge.\n","\n","**Technology and Techniques:**\n","\n","* You can use any technology or technique you prefer for data processing.\n","* We will value your knowledge of cloud platforms.\n","* If you use cloud platforms, follow the steps in your files WITHOUT adding access credentials to the different services.\n","\n","**Ranking Criteria:**\n","\n","* Challenges that are clearly organized, explanatory, modular, efficient, and creative will be ranked higher.\n","\n","**Assumptions and Documentation:**\n","\n","* Write down the assumptions you are making.\n","* Include the versions of the libraries you are using in the requirements.txt file.\n","* Do not delete what is already written in the requirements.txt file.\n","* For this challenge, we recommend that you clearly describe how each part of your exercise can be improved.\n","\n","**Data:**\n","\n","* You must use the data contained in the provided file.\n","* You can use the official Twitter documentation to understand the data structure.\n","\n","**Git Usage:**\n","\n","* We will positively evaluate good practices of Git usage.\n","* Use the main branch for any final version you want us to review.\n","* We recommend that you use some GitFlow practice.\n","* Do not delete your development branches.\n","\n","**Error Handling and Edge Cases:**\n","\n","* Consider error handling and edge cases.\n","\n","**Maintainability, Readability, and Scalability:**\n","\n","* Remember that you will be working with other developers, so the maintainability, readability, and scalability of your code is essential.\n","\n","**Code Documentation:**\n","\n","* Good code documentation always helps the reader.\n","\n","**Additional Notes:**\n","\n","* The `README.md` file mentions using GitHub Desktop, Git Flow, and Colab. These are tools that can be used for version control and code collaboration.\n","* The `README.md` file also mentions measuring time and memory. This can be done using Python's built-in `time` and `memory_profiler` modules."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"R45EuZ1JWSa4","executionInfo":{"status":"ok","timestamp":1712411981616,"user_tz":180,"elapsed":5,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6e51ab8c-7925-4c60-a101-3e22b602cc14"},"outputs":[{"output_type":"stream","name":"stdout","text":["All libraries where imported\n"]}],"source":["import sys\n","\n","if __name__ != \"__main__\":\n","    sys.exit()\n","\n","# General\n","import logging\n","import os\n","import time\n","\n","# Type related\n","from typing import List, Tuple, Any, Optional\n","import datetime\n","\n","# IO\n","import io\n","\n","# Google Drive\n","from google.colab import drive\n","\n","print(\"All libraries where imported\")"]},{"cell_type":"markdown","metadata":{"id":"hfr4Y-Cv8Cd9"},"source":["## **Definitions and Configurations:**\n","The following code snippet defines constants used in the data transfer and processing pipeline:\n","\n","- **Google Cloud Storage (GCS) Information:**\n","    - `BUCKET_NAME`: Specifies the name of the GCS bucket where data will be uploaded (`tw-gcp-public-lab`).\n","    - `FOLDER_NAME`: Denotes the folder within the bucket to store the uploaded file (`raw`).\n","    - `ZIP_FILE_NAME`: Represents the name of the compressed file containing tweets data (`tweets.json.zip`).\n","    - `GCS_SOURCE_URI`: Constructs the full URI for the file location in GCS after upload (`gs://tw-gcp-public-lab/raw/`).\n","\n","- **Local File Paths:**\n","    - `SOURCE_PATH`: Currently defines a local file path (`/content/drive/Othercomputers/My Mac/latam-challenge`), but it's not used in the provided code for downloading.\n","\n","- **Google Cloud Project and Dataset Information:**\n","    - `PROJECT_ID`: Specifies the Google Cloud project ID (`tw-techdash`).\n","    - `DATASET_NAME`: Defines the name of the BigQuery dataset where the data will be loaded (`tweets_dataset`).\n","    - `TABLE_NAME`: Identifies the name of the BigQuery table to store the extracted tweets data (`tweets`).\n","\n","**Observations:**\n","\n","- The `SOURCE_PATH` might require modification if you intend to download a file from a different location.\n","- Consider using environment variables or a configuration file to manage these constants, making your code more flexible and easier to maintain."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"g57ic23AMKFp","executionInfo":{"status":"ok","timestamp":1712411981617,"user_tz":180,"elapsed":5,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"79565bf6-a021-469f-c1c6-6669e164a404"},"outputs":[{"output_type":"stream","name":"stdout","text":["--------- Local Variables ---------\n","LC_START_TIME: 1712411981.4318748\n","LC_BUCKET_NAME: tw-gcp-public-lab\n","LC_FOLDER_NAME: raw\n","LC_ZIP_FILE_NAME: tweets.json.zip\n","LC_FILE_ID: 1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis\n","LC_GCS_SOURCE_URI: gs://tw-gcp-public-lab/raw/\n","LC_MOUNT_POINT: /content/drive\n","LC_SOURCE_PATH: /content/drive/Othercomputers/My Mac/latam-challenge\n","LC_PROJECT_ID: tw-techdash\n","LC_DATASET_NAME: tweets_dataset\n","LC_TABLE_NAME: tweets\n","LC_LOGGING_LEVEL: 10\n","LC_LOGGING_FILE: /content/drive/Othercomputers/My Mac/latam-challenge/latam-challenge.log\n"," ----------------------------------\n","Definitions and Configurations initialized\n"]}],"source":["# Definitions\n","\n","# Notebook time measure\n","START_TIME = os.environ['LC_START_TIME'] = str(time.time())\n","\n","# Google Cloud Storage (GCS) information\n","BUCKET_NAME = os.environ['LC_BUCKET_NAME'] = \"tw-gcp-public-lab\"\n","FOLDER_NAME = os.environ['LC_FOLDER_NAME'] = \"raw\"\n","ZIP_FILE_NAME = os.environ['LC_ZIP_FILE_NAME'] = \"tweets.json.zip\"\n","FILE_ID = os.environ['LC_FILE_ID'] = \"1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis\"\n","GCS_SOURCE_URI = os.environ['LC_GCS_SOURCE_URI'] = f\"gs://{BUCKET_NAME}/{FOLDER_NAME}/\"\n","\n","# Local file paths (consider user input/environment variables)\n","MOUNT_POINT = os.environ['LC_MOUNT_POINT'] = \"/content/drive\"\n","SOURCE_PATH = os.environ['LC_SOURCE_PATH'] = \"/content/drive/Othercomputers/My Mac/latam-challenge\"\n","\n","# Google Cloud project and dataset information (consider environment variables)\n","PROJECT_ID = os.environ['LC_PROJECT_ID'] = \"tw-techdash\"\n","DATASET_NAME = os.environ['LC_DATASET_NAME'] = \"tweets_dataset\"\n","TABLE_NAME = os.environ['LC_TABLE_NAME'] = \"tweets\"\n","\n","# Logging\n","LOGGING_LEVEL = os.environ['LC_LOGGING_LEVEL'] = str(logging.DEBUG)\n","LOGGING_FILE = os.environ['LC_LOGGING_FILE'] = f\"{SOURCE_PATH}/latam-challenge.log\"\n","\n","# Configurations\n","# Logging\n","logging.basicConfig(filename=LOGGING_FILE, level=int(LOGGING_LEVEL))\n","\n","def print_local_variables():\n","    \"\"\"\n","    Prints environment variables that begin with \"LC_\".\n","\n","    These variables often control language and locale-related settings.\n","\n","    Args: None\n","    \"\"\"\n","    variables_blacklist = ['LC_ALL']\n","\n","    print(\"--------- Local Variables ---------\")\n","    for key, value in os.environ.items():\n","        if key.startswith(\"LC_\") and (key not in variables_blacklist):\n","            print(f\"{key}: {value}\")\n","    print(\" ----------------------------------\")\n","\n","print_local_variables()\n","print(\"Definitions and Configurations initialized\")"]},{"cell_type":"markdown","metadata":{"id":"n0zd65ksIbea"},"source":["## **Jupyter Kernel code reloading**\n","\n","**Functionality:**\n","\n","* This code snippet utilizes magic commands within Jupyter Notebooks to manage code reloading.\n","* The `%reload_ext autoreload` line imports and activates the `autoreload` extension.\n","* The `%autoreload 2` line configures the `autoreload` extension to automatically reload Python modules when changes are detected.\n","\n","**Key Concepts:**\n","\n","* **Jupyter Magic Commands:** `%` prefix is used for magic commands that provide special functionality within Jupyter notebooks.\n","* **Autoreload Extension:**  A Jupyter extension that automatically reloads Python modules when changes are detected in the corresponding source files.\n","* **Reload Level:** The level `2` specifies that reload should occur when source files or any imported modules are modified (level 1 only reloads source file changes).\n","\n","**Overall Assessment:**\n","\n","* This code improves development efficiency within Jupyter notebooks by automatically reloading code, avoiding manual restarts.\n","* It leverages the `autoreload` extension for automatic reloading functionality.\n","* The configuration level `2` ensures comprehensive reloading behavior.\n","\n","**Potential Enhancements:**\n","\n","* While automatic reloading is helpful in development, it might not be suitable for production environments due to potential unexpected behavior during execution.\n","* Consider using this approach primarily for interactive development within Jupyter notebooks."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"fkniVfT9W5hr","executionInfo":{"status":"ok","timestamp":1712411981617,"user_tz":180,"elapsed":4,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bff91d1a-206d-4a5e-b3f0-9580b90c0d7b"},"outputs":[{"output_type":"stream","name":"stdout","text":["IPython autoreload activated\n"]}],"source":["# Enable automatic reloading of modules in Jupyter Notebook (improves development workflow)\n","%reload_ext autoreload\n","\n","# Automatically restart the kernel whenever the source code changes\n","# (Provides a clean development environment)\n","%autoreload 2\n","\n","print(\"IPython autoreload activated\")"]},{"cell_type":"markdown","metadata":{"id":"KcY78DdKJ1V_"},"source":["## **Google Drive mounting**\n","\n","**Functionality:**\n","\n","1. **Connects Google Drive:** This code establishes a connection between your Google Drive storage and the virtual machine running the Colab notebook.\n","2. **Navigates to Project Directory:** This magic command changes the working directory within the Colab notebook to a specific location within your project directory.\n","\n","**Key Concepts:**\n","\n","* **Google Drive Mounting:**\n","    - `from google.colab import drive`: Imports the `drive` module for interacting with Google Drive from Colab.\n","    - `drive.mount('/content/drive', force_remount=True)`: Mounts your Drive at the `/content/drive` path within Colab.\n","    - **Authorization:** Requires initial authorization to grant Colab access to your Drive.\n","* **Jupyter Notebook Magic Commands:**\n","    - `%cd`: A magic command specifically designed for changing directories.\n","\n","**Overall Assessment:**\n","\n","* **Convenient Data Access:** Enables seamless access to your personal data stored in Google Drive for use within Colab notebooks.\n","* **Improved Code Organization:** Helps organize your notebook within the project structure by focusing on a specific subdirectory (like \"src\").\n","\n","**Potential Enhancements:**\n","\n","* **Google Drive Mounting:**\n","    - **Error Handling:** Consider incorporating `try-except` blocks to gracefully handle potential mounting issues.\n","    - **Authentication Persistence:** Explore ways to persist the authentication token (if applicable) to avoid re-authorization for every session.\n","* **Navigation:**\n","    - **Clear Path Definitions:** Replace `{SOURCE_PATH}` with the actual path to your project directory for clarity.\n","    - **Error Handling:** Consider handling potential issues like non-existent directories using Python code (like `try-except` blocks).\n","\n","**Explanation:**\n","\n","1. **Mount Google Drive:** The first part of the code imports the `drive` module and mounts your Google Drive to the `/content/drive` directory within Colab. This allows you to access your Drive files from within your notebook.\n","2. **Change Directory:** The `%cd {SOURCE_PATH}/src` line uses a magic command to navigate to the subdirectory named \"src\" within your project directory (assuming `{SOURCE_PATH}` points to the correct location). This helps organize your notebook by focusing on the relevant project code.\n","\n","**Important Notes:**\n","\n","* Replace `{SOURCE_PATH}` with the actual path to your project directory on your machine.\n","* You'll need to go through an authorization process the first time you run the mounting code to grant Colab access to your Drive.\n","* It was not possible to have this"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"IV-79dMCpYAh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712411984960,"user_tz":180,"elapsed":3346,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"26206e0a-8897-471a-c880-071de0bd41af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Changed directory to: /content/drive/Othercomputers/My Mac/latam-challenge\n"]}],"source":["# Mount Google Drive (optional: call the function)\n","drive.mount(MOUNT_POINT, force_remount=True)\n","\n","# Change directory using a more explicit method\n","logging.info(f\"Changing directory to: {os.path.join(MOUNT_POINT, SOURCE_PATH, 'src')}\")\n","os.chdir(os.path.join(MOUNT_POINT, SOURCE_PATH, 'src'))\n","\n","print(f\"Changed directory to: {SOURCE_PATH}\")"]},{"cell_type":"markdown","metadata":{"id":"KuUJyVVpKQjJ"},"source":["## **Libraries requirements**\n","\n","**Functionality:**\n","\n","- **Installs Python Libraries:** This code snippet installs a collection of Python libraries listed in a file named `requirements.txt` within the currently active virtual environment.\n","\n","**Key Concepts:**\n","\n","- **requirements.txt File:** This text file contains a list of library names and their version requirements, ensuring consistent installation across environments.\n","- **Virtual Environments:** Virtual environments isolate project dependencies, preventing conflicts with other Python projects on your system.\n","- **sys.executable:** This Python variable points to the path of the Python interpreter for the active virtual environment.\n","- **pip:** The Python Package Installer (pip) is used for managing Python packages and libraries.\n","\n","**Explanation:**\n","\n","1. **`import sys`**: Imports the `sys` module, providing access to system-specific variables and functions.\n","2. **`!{sys.executable} -m pip install -r '../requirements.txt'`**: This line calls the pip installer within the virtual environment:\n","   - **`!`**: Jupyter Notebook magic command to execute terminal commands.\n","   - **`{sys.executable}`**: Ensures pip is called from the virtual environment's Python interpreter.\n","   - **`-m`**: Designates a module to execute as a script (in this case, `pip`).\n","   - **`install -r`**: Instructs pip to install packages from a requirements file.\n","   - **`'../requirements.txt'`**: Specifies the path to the requirements file (relative to the current notebook's directory).\n","\n","**Important Notes:**\n","\n","- **Virtual Environment Activation:** Ensure you've activated the desired virtual environment before running this code.\n","- **Path to requirements.txt:** Verify that `../requirements.txt` correctly points to the file's location.\n","- **Internet Connection:** An internet connection is required for pip to download and install packages.\n","\n","**Overall Assessment:**\n","\n","- **Efficient Dependency Management:** Using `requirements.txt` is a best practice for managing project dependencies consistently.\n","- **Consistent Environments:** Facilitates consistent library installations across different machines for reproducibility.\n","- **Collaboration:** Enables easy setup of the same project environment for others.\n","\n","**Potential Enhancements:**\n","\n","- **Error Handling:** Consider incorporating error handling (like try-except blocks) to gracefully handle potential issues during installation, such as network connectivity problems or missing packages.\n"]},{"cell_type":"code","source":["%run libraries.py\n","install_requirements\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"id":"2SS1z03vnz0d","executionInfo":{"status":"ok","timestamp":1712411984961,"user_tz":180,"elapsed":5,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"de55638d-af5f-495a-94df-305b87528a42"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function __main__.install_requirements(requirements_path: str = '../requirements.txt') -> None>"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>install_requirements</b><br/>def install_requirements(requirements_path: str=&#x27;../requirements.txt&#x27;) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/drive/Othercomputers/My Mac/latam-challenge/src/libraries.py</a>Installs Python libraries from the specified requirements file.\n","\n","Args:\n","    requirements_path (str, optional): Path to the requirements.txt file. Defaults to &quot;../requirements.txt&quot;.\n","\n","Returns:\n","    None</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 3);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"V_d7gErJW5hs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712411997533,"user_tz":180,"elapsed":12200,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"579f54a8-0b71-4f6c-faa3-260d966e08bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully installed libraries from requirements.txt\n"]}],"source":["# Call the install function\n","install_requirements()\n"]},{"cell_type":"code","source":["#Google Cloud Platform\n","from google.cloud import storage\n","from google.cloud import bigquery\n","\n","# Time and Memory\n","import cProfile\n","import memory_profiler as mp"],"metadata":{"id":"3-fOVIZbioi5","executionInfo":{"status":"ok","timestamp":1712411998871,"user_tz":180,"elapsed":1341,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PA2wPhkO7khO"},"source":["## **Ingest Google Drive ZIP into Google Storage**\n","**Key Functions:**\n","\n","1. **authenticate_google_drive()**: Authenticates with Google Drive using the user's credentials.\n","2. **download_file_from_drive(drive_service, file_id)**: Downloads a specified file from Google Drive.\n","3. **upload_file_to_cloud_storage(client, bucket_name, folder_name, downloaded, zip_file_name)**: Uploads a file to Google Cloud Storage, handling folder creation if needed.\n","4. **decompress_zip_file(client, bucket_name, folder_name, zip_file_name)**: Decompresses a ZIP file within a GCS bucket.\n","\n","**Code Structure:**\n","\n","- **Logging:** Employs `logging` for debugging and tracking progress.\n","- **Error Handling:** Uses try-except blocks to gracefully handle potential errors.\n","- **Modularity:** Separates functionality into distinct, reusable functions.\n","- **Type Hints:** Enhances code readability and potential type checking.\n","\n","**Main Code Execution:**\n","\n","1. Configures logging to a file named 'transfer.log'.\n","2. Authenticates with Google Drive.\n","3. Downloads the specified file from Drive.\n","4. Creates a Cloud Storage client.\n","5. Uploads the downloaded file to GCS.\n","6. Decompresses the ZIP file in GCS if its content type is 'application/zip'.\n","7. Logs success or failure messages.\n","8. Finally, ensures the downloaded file is closed.\n","\n","**Overall Assessment:**\n","\n","- **Well-structured:** The code is organized, modular, and includes error handling.\n","- **Clear Functionality:** It effectively handles file transfer and decompression tasks.\n","- **Authentication Flexibility:** Uses authentication methods external to the code (useful for avoiding credentials in code).\n","- **Good Practices:** Adheres to good practices like logging and try-except blocks.\n","\n","**Potential Enhancements:**\n","\n","- **Parameterization:** Explore using command-line arguments or configuration files to adjust parameters more flexibly.\n","- **Progress Reporting:** Consider more granular progress reporting for downloads/uploads.\n","- **Content Validation:** Validate file content after decompression for integrity.\n","- **Advanced Error Handling:** Implement retries or alternative actions for potential errors.\n","\n","This code provides a foundation for file transfer and decompression tasks within Google Cloud environments, demonstrating clarity and attention to best practices."]},{"cell_type":"code","source":["%run measure.py\n","%run ingest.py\n","print(storage)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nObA69j8mp5W","executionInfo":{"status":"ok","timestamp":1712411999244,"user_tz":180,"elapsed":374,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"837eb3b6-10b5-4d09-d139-8d55a51146fa"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["<module 'google.cloud.storage' from '/usr/local/lib/python3.10/dist-packages/google/cloud/storage/__init__.py'>\n"]}]},{"cell_type":"code","source":["authenticate_google_drive\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":174},"id":"7JcRoCPinNre","executionInfo":{"status":"ok","timestamp":1712411999564,"user_tz":180,"elapsed":322,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"5e7332d7-41d1-476d-dd3d-7ca9554814f8"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function __main__.authenticate_google_drive() -> None>"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>authenticate_google_drive</b><br/>def authenticate_google_drive() -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/drive/Othercomputers/My Mac/latam-challenge/src/ingest.py</a>Authenticates to Google Drive using the user&#x27;s credentials.\n","\n","Args:\n","    None</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 9);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["download_file_from_drive\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"id":"0f2oSUc9nNt3","executionInfo":{"status":"ok","timestamp":1712411999565,"user_tz":180,"elapsed":6,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"d781b9b3-34d9-497f-bb60-93997d8a2f11"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function __main__.download_file_from_drive(drive_service: Any, file_id: str) -> _io.BytesIO>"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>download_file_from_drive</b><br/>def download_file_from_drive(drive_service: Any, file_id: str) -&gt; io.BytesIO</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/drive/Othercomputers/My Mac/latam-challenge/src/ingest.py</a>Downloads a file from Google Drive.\n","\n","Args:\n","    drive_service (Any): The Google Drive service resource (in the build function returns Any).\n","    file_id (str): The ID of the file to download.\n","\n","Returns:\n","    io.BytesIO: The downloaded file content as a BytesIO object.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 22);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["upload_file_to_cloud_storage\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"fgs-aX-bnNwF","executionInfo":{"status":"ok","timestamp":1712411999565,"user_tz":180,"elapsed":5,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"09347a65-2f6a-4ab0-f460-0da53a032cdf"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function __main__.upload_file_to_cloud_storage(bucket: google.cloud.storage.bucket.Bucket, folder_name: str, downloaded: _io.BytesIO, zip_file_name: str) -> google.cloud.storage.blob.Blob>"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>upload_file_to_cloud_storage</b><br/>def upload_file_to_cloud_storage(bucket: storage.Bucket, folder_name: str, downloaded: io.BytesIO, zip_file_name: str) -&gt; storage.Blob</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/drive/Othercomputers/My Mac/latam-challenge/src/ingest.py</a>Uploads a file to Google Cloud Storage.\n","\n","Args:\n","    bucket (Bucket): Google Cloud Storage bucket.\n","    folder_name (str): The name of the folder within the bucket where the file will be uploaded.\n","    downloaded (io.BytesIO): The downloaded file to upload.\n","    zip_file_name (str): The name of the file to be uploaded.\n","\n","Returns:\n","    google.cloud.storage.Blob: The uploaded blob object.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 49);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["decompress_zip_file\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"id":"XtJNiZRgnNyn","executionInfo":{"status":"ok","timestamp":1712411999565,"user_tz":180,"elapsed":5,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"54763639-d063-423d-e147-da4f2886fafe"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function __main__.decompress_zip_file(bucket: google.cloud.storage.bucket.Bucket, folder_name: str, zip_file_name: str) -> str>"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>decompress_zip_file</b><br/>def decompress_zip_file(bucket: storage.Bucket, folder_name: str, zip_file_name: str) -&gt; str</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/drive/Othercomputers/My Mac/latam-challenge/src/ingest.py</a>Decompresses a ZIP file stored in Google Cloud Storage.\n","\n","Args:\n","    bucket (Bucket): Google Cloud Storage bucket where the ZIP file is stored.\n","    folder_name (str): The name of the folder within the bucket where the ZIP file is located.\n","    zip_file_name (str): The name of the ZIP file\n","\n","Returns:\n","    str: The unzipped file name.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 77);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","execution_count":13,"metadata":{"id":"pz4EZrB4W5hu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"385d8de6-1678-46d3-8f4e-2f1184b47eb7","executionInfo":{"status":"ok","timestamp":1712412017516,"user_tz":180,"elapsed":17955,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading 100%\n","File uploaded to gs://tw-gcp-public-lab/raw/tweets.json.zip\n","File decompressed in gs://tw-gcp-public-lab/raw/farmers-protest-tweets-2021-2-4.json\n","File transfer process completed.\n"]}],"source":["downloaded: io.BytesIO = io.BytesIO()\n","\n","try:\n","    # Authenticate to Google Drive\n","    authenticate_google_drive()\n","    drive_service: Any = build('drive', 'v3')\n","\n","    # Download file from Drive\n","    downloaded: io.BytesIO = download_file_from_drive(drive_service, FILE_ID)\n","\n","    # Create Cloud Storage Bucket\n","    bucket: storage.Bucket = storage.Client().bucket(BUCKET_NAME)\n","\n","    # Upload file to Cloud Storage\n","    uploaded_blob: storage.Blob = upload_file_to_cloud_storage(bucket, FOLDER_NAME, downloaded, ZIP_FILE_NAME)\n","\n","    # Decompress ZIP file if applicable\n","    if uploaded_blob.content_type == 'application/zip':\n","        json_file_name: str = decompress_zip_file(bucket, FOLDER_NAME, ZIP_FILE_NAME)\n","\n","    logging.info(\"File transfer successful!\")\n","\n","except Exception as e:\n","    logging.error(f\"An error occurred: {e}\")\n","\n","finally:\n","    downloaded.close()  # Close downloaded file\n","    print(\"File transfer process completed.\")\n"]},{"cell_type":"markdown","metadata":{"id":"Dh8NblbcCU8H"},"source":["## **BigQuery Storage Functions**\n","\n","**Functionality:**\n","\n","These Python functions interact with BigQuery to authenticate, create datasets and tables, and load data from Cloud Storage.\n","\n","**Key Concepts:**\n","\n","* **Client:** The `bigquery.Client` object is central to interacting with BigQuery.\n","* **Datasets and Tables:** Datasets organize tables, and both can be created or overwritten using these functions.\n","* **Data Loading:** Data is loaded from Cloud Storage in newline-delimited JSON format, and BigQuery automatically infers the schema.\n","* **Error Handling:** The functions use logging and try-except blocks to handle errors and provide informative messages.\n","\n","**Snippet 1: authenticate_bigquery**\n","\n","**Functionality:**\n","\n","Authenticates to BigQuery and returns a client object for subsequent operations.\n","\n","**Key Concepts:**\n","\n","* **Project ID:** Required for authentication.\n","\n","**Overall Assessment:**\n","\n","Clear and concise function for initial setup.\n","\n","**Potential Enhancements:**\n","\n","* **Error Handling:** Consider logging errors with more detail.\n","\n","**Snippet 2: create_dataset**\n","\n","**Functionality:**\n","\n","Creates a dataset if it doesn't exist or overwrites it if specified.\n","\n","**Key Concepts:**\n","\n","* **Mode:** Optional argument to control actions if the dataset already exists.\n","\n","**Overall Assessment:**\n","\n","Good flexibility with `mode` argument for handling existing datasets.\n","\n","**Potential Enhancements:**\n","\n","* **Input Validation:** Consider validating dataset names for compliance with BigQuery rules.\n","\n","**Snippet 3: create_table**\n","\n","**Functionality:**\n","\n","Creates a table within a dataset if it doesn't exist or overwrites it if specified.\n","\n","**Key Concepts:**\n","\n","* **Schema Inference:** Uses an empty schema to let BigQuery infer it from the data.\n","\n","**Overall Assessment:**\n","\n","Handles table creation effectively.\n","\n","**Potential Enhancements:**\n","\n","* **Schema Definition:** Explore allowing optional schema definition for more control.\n","\n","**Snippet 4: load_data_from_storage**\n","\n","**Functionality:**\n","\n","Loads data from a newline-delimited JSON file in Cloud Storage to a BigQuery table.\n","\n","**Key Concepts:**\n","\n","* **Load Job Configuration:** Specifies data format, schema inference, and handling of unknown values.\n","\n","**Overall Assessment:**\n","\n","Well-structured data loading process.\n","\n","**Potential Enhancements:**\n","\n","* **Progress Reporting:** Consider logging loading progress.\n","* **Data Validation:** Explore adding data validation checks before loading.\n"]},{"cell_type":"code","source":["%run storage.py\n","print(storage)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tTYu-RIEim3Q","executionInfo":{"status":"ok","timestamp":1712412017516,"user_tz":180,"elapsed":14,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"91ad8b58-a217-4227-b775-7fa949fd5c7f"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["<module 'google.cloud.storage' from '/usr/local/lib/python3.10/dist-packages/google/cloud/storage/__init__.py'>\n"]}]},{"cell_type":"code","source":["authenticate_bigquery\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"id":"kLNuqnQMiv8n","executionInfo":{"status":"ok","timestamp":1712412017516,"user_tz":180,"elapsed":12,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"a95ed721-7db0-44ec-da6c-051bd8f462e2"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function __main__.authenticate_bigquery(project_id: str) -> google.cloud.bigquery.client.Client>"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>authenticate_bigquery</b><br/>def authenticate_bigquery(project_id: str) -&gt; bigquery.Client</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/drive/Othercomputers/My Mac/latam-challenge/src/storage.py</a>Authenticates to BigQuery and returns the client object.\n","\n","Args:\n","    project_id (str): Your GCP project ID.\n","\n","Returns:\n","    bigquery.Client: BigQuery client object.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 5);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["create_dataset\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"id":"zAPjRXySiv_e","executionInfo":{"status":"ok","timestamp":1712412017516,"user_tz":180,"elapsed":11,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"11fd2fa8-6891-4807-f9e7-18fb705599dc"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function __main__.create_dataset(client: google.cloud.bigquery.client.Client, dataset_name: str, mode: str = 'create') -> None>"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>create_dataset</b><br/>def create_dataset(client: bigquery.Client, dataset_name: str, mode: str=&#x27;create&#x27;) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/drive/Othercomputers/My Mac/latam-challenge/src/storage.py</a>Creates a BigQuery dataset if it doesn&#x27;t exist.\n","\n","Args:\n","    client (bigquery.Client): BigQuery client object.\n","    dataset_name (str): Name of the dataset to create.\n","    mode (Optional[str], optional): Action to take if the dataset already exists (&#x27;create&#x27; or &#x27;overwrite&#x27;). Defaults to &#x27;create&#x27;.\n","\n","Raises:\n","    Exception: For unexpected errors during dataset creation or existence check.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 20);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["create_table\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"aObeNTAdiwCs","executionInfo":{"status":"ok","timestamp":1712412017516,"user_tz":180,"elapsed":11,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"a27db280-c278-43f1-eb11-340ef3e96a18"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function __main__.create_table(client: google.cloud.bigquery.client.Client, dataset_name: str, table_name: str, mode: str = 'create') -> None>"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>create_table</b><br/>def create_table(client: bigquery.Client, dataset_name: str, table_name: str, mode: str=&#x27;create&#x27;) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/drive/Othercomputers/My Mac/latam-challenge/src/storage.py</a>Creates a BigQuery table if it doesn&#x27;t exist.\n","\n","Args:\n","    client (bigquery.Client): BigQuery client object.\n","    dataset_name (str): Name of the dataset containing the table.\n","    table_name (str): Name of the table to create.\n","    mode (Optional[str], optional): Action to take if the table already exists (&#x27;create&#x27; or &#x27;overwrite&#x27;). Defaults to &#x27;create&#x27;.\n","\n","Raises:\n","    Exception: For unexpected errors during table creation or existence check.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 54);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["load_data_from_storage\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"znSn8XSEiwF7","executionInfo":{"status":"ok","timestamp":1712412017516,"user_tz":180,"elapsed":10,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"2fce6581-3652-4d7b-c78e-4836e6029e10"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function __main__.load_data_from_storage(client: google.cloud.bigquery.client.Client, source_uri: str, dataset_name: str, table_name: str, json_file_name: str) -> None>"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>load_data_from_storage</b><br/>def load_data_from_storage(client: bigquery.Client, source_uri: str, dataset_name: str, table_name: str, json_file_name: str) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/drive/Othercomputers/My Mac/latam-challenge/src/storage.py</a>Loads data from Cloud Storage (newline-delimited JSON) to BigQuery table.\n","\n","Args:\n","    client (bigquery.Client): BigQuery client object.\n","    source_uri (str): URI of the data file in Cloud Storage.\n","    dataset_name (str): Name of the dataset containing the table.\n","    table_name (str): Name of the table to load data into.\n","    json_file_name (str): Name of the JSON file in the Bucket\n","Raises:\n","    Exception: For unexpected errors during data loading.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 90);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","execution_count":19,"metadata":{"id":"teRz4l7jW-fq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712412039748,"user_tz":180,"elapsed":22243,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"403072dc-8e4c-4936-841c-67b4023b7614"},"outputs":[{"output_type":"stream","name":"stdout","text":["<google.cloud.bigquery.client.Client object at 0x7c60952a33a0>\n","Overwrite dataset tweets_dataset\n","Overwrite table tweets_dataset.tweets\n","Store gs://tw-gcp-public-lab/raw/farmers-protest-tweets-2021-2-4.json into BigQuery tweets_dataset.tweets\n","Data storage completed!\n"]}],"source":["# Authenticate to BigQuery (assuming PROJECT_ID is defined elsewhere)\n","bigquery_client: bigquery.Client = authenticate_bigquery(PROJECT_ID)\n","print(bigquery_client)\n","\n","# Create dataset (overwrite if needed)\n","create_dataset(bigquery_client, DATASET_NAME, mode='overwrite')\n","print(f\"Overwrite dataset {DATASET_NAME}\")\n","\n","# Create table (overwrite if needed)\n","create_table(bigquery_client, DATASET_NAME, TABLE_NAME, mode='overwrite')\n","print(f\"Overwrite table {DATASET_NAME}.{TABLE_NAME}\")\n","\n","# Load data from Cloud Storage\n","load_data_from_storage(bigquery_client, GCS_SOURCE_URI, DATASET_NAME, TABLE_NAME, json_file_name)\n","print(f\"Store {GCS_SOURCE_URI}{json_file_name} into BigQuery {DATASET_NAME}.{TABLE_NAME}\")\n","\n","print(\"Data storage completed!\")"]},{"cell_type":"markdown","metadata":{"id":"dEcJqRweD26B"},"source":["## **BigQuery Processing Functions**\n","\n","**Functionality:**\n","\n","- **Processes BigQuery Results:** The `process_bigquery_results` function executes the query, handles results, and converts them into a desired format (list of tuples with date and username).\n","\n","**Key Concepts:**\n","\n","- **Type Hints:** Employs type hints (`List`, `Tuple`, `datetime.date`) for improved code readability and potential static type checking.\n","- **Error Handling:** Incorporates `try-except` blocks to gracefully handle exceptions (`BadRequest` and generic exceptions).\n","- **Data Conversion:** Converts retrieved data rows into the specified format.\n","\n","**Overall Assessment:**\n","\n","- **Clear Separation:** Functions promote modularity and reusability.\n","- **Meaningful Variable Names:** Descriptive names enhance code understandability.\n","- **Error Management:** Handles potential errors during query execution and processing.\n","\n","**Potential Enhancements:**\n","\n","- **Input Validation:** Consider validating the constructed query string before execution.\n","- **Logging:** Integrate logging for detailed tracking and debugging.\n","- **Security:** Ensure secure credential management for BigQuery access.\n","- **Query Parameterization:** If DATASET_NAME and TABLE_NAME are not intended for hardcoding, utilize BigQuery's query parameters for better reusability and security.\n","- **Data Usage:** Currently, the extracted data is printed. You can modify this section to store the data in a desired location or perform further processing.\n","\n","This code provides a foundation for working with BigQuery data retrieval and processing. You can extend it based on your specific needs."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"WLlwchwVZqYx","colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"status":"ok","timestamp":1712412039749,"user_tz":180,"elapsed":29,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"88d18211-66c5-4656-d375-2b81489700a5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function __main__.process_bigquery_results(client: google.cloud.bigquery.client.Client, query: str) -> List[Tuple[Any, Any]]>"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>process_bigquery_results</b><br/>def process_bigquery_results(client: bigquery.Client, query: str) -&gt; List[Tuple[Any, Any]]</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/drive/Othercomputers/My Mac/latam-challenge/src/processing.py</a>Executes a BigQuery query, handles results, and performs data conversion.\n","\n","Args:\n","    client: BigQuery client object.\n","    query: BigQuery SQL query string.\n","\n","Returns:\n","    A list of tuples containing the extracted data (date and username).\n","\n","Raises:\n","    NotFound: If the query returns no results.\n","    Exception: For other unexpected errors during query execution or processing.</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 5);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":20}],"source":["%run processing.py\n","process_bigquery_results\n"]},{"cell_type":"markdown","metadata":{"id":"5xm-D9ctF0Nx"},"source":["## **BigQuery Queries**\n","\n","**Snippet 1: Top 10 Dates with Top Users**\n","\n","**Functionality**\n","\n","This SQL query identifies the top 10 dates with the most tweets and, for each of those dates, finds the user with the most tweets (considering usernames alphabetically in case of ties).\n","\n","**Key Concepts**\n","\n","* **Common Table Expressions (CTEs):** The query utilizes two CTEs:\n","    * `TopDates`: Calculates the daily tweet count and ranks them in descending order, selecting the top 10.\n","    * `TopUsersDate`: Joins the `tweets` table with `TopDates` to find the user(s) with the most tweets for each top date. It uses `ROW_NUMBER()` to handle ties by username order.\n","* **Window Functions:** `ROW_NUMBER()` is used within `TopUsersDate` to assign a unique row number within each date partition, ordered by tweet count (descending) and then by number of tweets per user (descending).\n","* **Filtering:** The final result retrieves users with `row_number = 1` (the user with the most tweets for each date).\n","\n","**Overall Assessment**\n","\n","This query effectively addresses the task by leveraging CTEs for modularity and window functions to handle ranking and ties.\n","\n","**Potential Enhancements**\n","\n","* **Clarity:** Consider adding comments within the query to explain the purpose of each CTE.\n","* **Efficiency:** Explore alternative approaches to handle ties if performance is critical.\n","\n","**Data Usage**\n","\n","The query currently prints the `tweets_date` and `username`. You might want to consider storing this information in a table or using it for further analysis.\n","\n","**Snippet 2: Top 10 Most Used Emojis**\n","\n","**Functionality**\n","\n","This query extracts emojis from tweets and identifies the top 10 most frequently used emojis along with their counts.\n","\n","**Key Concepts**\n","\n","* **Regular Expressions (RegEx):** The `REGEXP_EXTRACT_ALL()` function utilizes a complex RegEx pattern to capture a wide range of emoji characters across different Unicode blocks.\n","* **UNNEST:** The `UNNEST()` operator is used to explode the extracted emoji list into a single row per emoji for counting.\n","\n","**Overall Assessment**\n","\n","This query effectively extracts and counts emojis, providing valuable insights into emoji usage.\n","\n","**Potential Enhancements**\n","\n","* **Filtering:** Depending on the analysis goals, you might want to filter out specific emoji categories (e.g., flags, country codes).\n","* **Normalization:** Consider normalizing emojis to a canonical form to handle variations (e.g., skin tone modifiers).\n","\n","**Data Usage**\n","\n","The query currently prints the `emoji` and `count`. You could store this information for further analysis of emoji popularity.\n","\n","**Snippet 3: Top 10 Influential Users**\n","\n","**Functionality**\n","\n","This query identifies the top 10 users with the most mentions (`@username`) received in tweets.\n","\n","**Key Concepts**\n","\n","* **UNNEST:** Similar to snippet 2, `UNNEST()` is used to explode the mentioned user list from each tweet for counting mentions.\n","\n","**Overall Assessment**\n","\n","This query effectively identifies influential users based on mentions.\n","\n","**Potential Enhancements**\n","\n","* **Filtering:** You might consider filtering out self-mentions or mentions from specific accounts.\n","* **Weighted Mentions:** Depending on the analysis goals, explore assigning weights to mentions based on factors like follower count.\n","\n","**Data Usage**\n","\n","The query currently prints the `username` and `mention_count`. You could store this information for further analysis of user influence."]},{"cell_type":"markdown","metadata":{"id":"kZOeNPrCbbU5"},"source":[]},{"cell_type":"code","execution_count":21,"metadata":{"id":"kKjm82w1ES1F","executionInfo":{"status":"ok","timestamp":1712412039749,"user_tz":180,"elapsed":26,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}}},"outputs":[],"source":["# Challenge py files\n","import queries\n","%run q1_time.py\n","%run q2_time.py\n","%run q3_time.py\n","%run q1_memory.py\n","%run q2_memory.py\n","%run q3_memory.py"]},{"cell_type":"code","source":["print(\"LATAM Challenge Time - Top 10 Dates with more Tweets and the Username with more Tweets for each Day\")\n","q1_time(bigquery_client, queries.top_dates_with_top_users)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zvJceQYCuH1e","executionInfo":{"status":"ok","timestamp":1712412040882,"user_tz":180,"elapsed":1159,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"b8647147-4158-4623-e8c9-d4f2198b3e33"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["LATAM Challenge Time - Top 10 Dates with more Tweets and the Username with more Tweets for each Day\n"]},{"output_type":"execute_result","data":{"text/plain":["[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n"," (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n"," (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n"," (datetime.date(2021, 2, 16), 'jot__b'),\n"," (datetime.date(2021, 2, 14), 'rebelpacifist'),\n"," (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n"," (datetime.date(2021, 2, 15), 'jot__b'),\n"," (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n"," (datetime.date(2021, 2, 23), 'Surrypuria'),\n"," (datetime.date(2021, 2, 19), 'Preetm91')]"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["print(\"LATAM Challenge Time - Top 10 Dates Emojis\")\n","q2_time(bigquery_client, queries.top_emojis)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l1c5mz74uH-J","executionInfo":{"status":"ok","timestamp":1712412042150,"user_tz":180,"elapsed":1271,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"664c283a-7564-463c-e5b9-65dda93e58e5"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["LATAM Challenge Time - Top 10 Dates Emojis\n"]},{"output_type":"execute_result","data":{"text/plain":["[('✊', 2402),\n"," ('❤️', 1382),\n"," ('❤', 397),\n"," ('☮️', 316),\n"," ('♂️', 179),\n"," ('✌️', 168),\n"," ('♀️', 148),\n"," ('✌', 106),\n"," ('‼️', 74),\n"," ('♥️', 73)]"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["print(\"LATAM Challenge Time - Top 10 Influential Users\")\n","q3_time(bigquery_client, queries.top_influential_users)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pSfoZ8C0uIE0","executionInfo":{"status":"ok","timestamp":1712412043411,"user_tz":180,"elapsed":1264,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"739c79db-ceac-4900-d497-b529b5b8994f"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["LATAM Challenge Time - Top 10 Influential Users\n"]},{"output_type":"execute_result","data":{"text/plain":["[('narendramodi', 2265),\n"," ('Kisanektamorcha', 1840),\n"," ('RakeshTikaitBKU', 1644),\n"," ('PMOIndia', 1427),\n"," ('RahulGandhi', 1146),\n"," ('GretaThunberg', 1048),\n"," ('RaviSinghKA', 1019),\n"," ('rihanna', 986),\n"," ('UNHumanRights', 962),\n"," ('meenaharris', 926)]"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["print(\"LATAM Challenge Memory - Top 10 Dates with more Tweets and the Username with more Tweets for each Day\")\n","print(q1_memory(bigquery_client, queries.top_dates_with_top_users), end=\"\\n\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0kmxXSGImMVa","executionInfo":{"status":"ok","timestamp":1712412044513,"user_tz":180,"elapsed":1107,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"e82f8bae-caf2-486f-a9f6-a5880edb96ee"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["\n","PYDEV DEBUGGER WARNING:\n","sys.settrace() should not be used when the debugger is being used.\n","This may cause the debugger to stop working correctly.\n","If this is needed, please check: \n","http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n","to see how to restore the debug tracing back correctly.\n","Call Location:\n","  File \"/usr/local/lib/python3.10/dist-packages/memory_profiler.py\", line 847, in enable\n","    sys.settrace(self.trace_memory_usage)\n","\n"]},{"output_type":"stream","name":"stdout","text":["LATAM Challenge Memory - Top 10 Dates with more Tweets and the Username with more Tweets for each Day\n"]},{"output_type":"stream","name":"stderr","text":["\n","PYDEV DEBUGGER WARNING:\n","sys.settrace() should not be used when the debugger is being used.\n","This may cause the debugger to stop working correctly.\n","If this is needed, please check: \n","http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n","to see how to restore the debug tracing back correctly.\n","Call Location:\n","  File \"/usr/local/lib/python3.10/dist-packages/memory_profiler.py\", line 850, in disable\n","    sys.settrace(self._original_trace_function)\n","\n"]},{"output_type":"stream","name":"stdout","text":["Filename: /content/drive/Othercomputers/My Mac/latam-challenge/src/q1_memory.py\n","\n","Line #    Mem usage    Increment  Occurrences   Line Contents\n","=============================================================\n","     7    198.9 MiB    198.9 MiB           1   @profile\n","     8                                         def q1_memory(\n","     9                                             client: bigquery.Client,\n","    10                                             query: str\n","    11                                         ) -> List[Tuple[datetime.date, str]]:\n","    12                                             \"\"\"\n","    13                                             Executes a BigQuery SQL query and returns a list of tuples containing dates and strings extracted from the results.\n","    14                                         \n","    15                                             Args:\n","    16                                                 client (bigquery.Client): A BigQuery client object.\n","    17                                                 query (str): The BigQuery SQL query to be executed.\n","    18                                         \n","    19                                             Returns:\n","    20                                                 List[Tuple[datetime.date, str]]: A list of tuples where each tuple contains a datetime.date object and a string.\n","    21                                             \"\"\"\n","    22                                         \n","    23    198.9 MiB      0.0 MiB           1       return process_bigquery_results(client, query)\n","\n","\n","[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n","\n"]}]},{"cell_type":"code","source":["print(\"LATAM Challenge Memory - Top 10 Dates Emojis\")\n","print(q2_memory(bigquery_client, queries.top_emojis), end=\"\\n\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O9x_6AOpwGzG","executionInfo":{"status":"ok","timestamp":1712412045336,"user_tz":180,"elapsed":830,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"78309e59-9581-4db9-b729-7d9cb3c6cf7a"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["LATAM Challenge Memory - Top 10 Dates Emojis\n","Filename: /content/drive/Othercomputers/My Mac/latam-challenge/src/q2_memory.py\n","\n","Line #    Mem usage    Increment  Occurrences   Line Contents\n","=============================================================\n","     7    198.9 MiB    198.9 MiB           1   @profile\n","     8                                         def q2_memory(\n","     9                                             client: bigquery.Client,\n","    10                                             query: str\n","    11                                         ) -> List[Tuple[str, int]]:\n","    12                                             \"\"\"\n","    13                                             Executes a BigQuery SQL query and returns a list of tuples containing dates and strings extracted from the results.\n","    14                                         \n","    15                                             Args:\n","    16                                                 client (bigquery.Client): A BigQuery client object.\n","    17                                                 query (str): The BigQuery SQL query to be executed.\n","    18                                         \n","    19                                             Returns:\n","    20                                                 List[Tuple[datetime.date, str]]: A list of tuples where each tuple contains a datetime.date object and a string.\n","    21                                             \"\"\"\n","    22                                         \n","    23    198.9 MiB      0.0 MiB           1       return process_bigquery_results(client, query)\n","\n","\n","[('✊', 2402), ('❤️', 1382), ('❤', 397), ('☮️', 316), ('♂️', 179), ('✌️', 168), ('♀️', 148), ('✌', 106), ('‼️', 74), ('♥️', 73)]\n","\n"]}]},{"cell_type":"code","source":["print(\"LATAM Challenge Memory - Top 10 Influential Users\")\n","print(q3_memory(bigquery_client, queries.top_influential_users), end=\"\\n\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wulk-FtXwG1f","executionInfo":{"status":"ok","timestamp":1712412046314,"user_tz":180,"elapsed":985,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"1ede1072-5f16-445b-df83-1c29b4c6f581"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["LATAM Challenge Memory - Top 10 Influential Users\n","Filename: /content/drive/Othercomputers/My Mac/latam-challenge/src/q3_memory.py\n","\n","Line #    Mem usage    Increment  Occurrences   Line Contents\n","=============================================================\n","     7    198.9 MiB    198.9 MiB           1   @profile\n","     8                                         def q3_memory(\n","     9                                             client: bigquery.Client,\n","    10                                             query: str\n","    11                                         ) -> List[Tuple[str, int]]:\n","    12                                             \"\"\"\n","    13                                             Executes a BigQuery SQL query and returns a list of tuples containing dates and strings extracted from the results.\n","    14                                         \n","    15                                             Args:\n","    16                                                 client (bigquery.Client): A BigQuery client object.\n","    17                                                 query (str): The BigQuery SQL query to be executed.\n","    18                                         \n","    19                                             Returns:\n","    20                                                 List[Tuple[datetime.date, str]]: A list of tuples where each tuple contains a datetime.date object and a string.\n","    21                                             \"\"\"\n","    22                                         \n","    23    198.9 MiB      0.0 MiB           1       return process_bigquery_results(client, query)\n","\n","\n","[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n","\n"]}]},{"cell_type":"code","execution_count":28,"metadata":{"id":"Kb9SWakoNiJe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712412046314,"user_tz":180,"elapsed":9,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"432b965b-1630-471d-aea6-c7989feb169f"},"outputs":[{"output_type":"stream","name":"stdout","text":["The notebook execution ended\n","Elapsed time in the notebook: 1.08 minutes\n"]}],"source":["%run measure.py\n","\n","# Measure elapsed time in notebook\n","if 'end_time' in locals():\n","    print(\"The notebook execution already ended\")\n","else:\n","    print(\"The notebook execution ended\")\n","    end_time = measure_notebook_elapsed_time(START_TIME)\n","\n","print_notebook_elapsed_time(end_time)  # Print the result"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":0}