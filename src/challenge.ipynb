{"cells":[{"cell_type":"markdown","metadata":{"id":"jVGOHFL3W5hp"},"source":["## **LATAM Challenge - Data Ingest, Storage and Processing with Google Drive, Google Cloud Storage and Google BigQuery in Google Colab (Jupyter) with Python 3.10**\n","\n","**Welcome to the Data Engineer Challenge.** On this occasion, you will have the opportunity to get closer to the reality of the role, demonstrate your skills and knowledge in data processing with Python and different data structures.\n","\n","**Preparation:**\n","\n","1. Initial project commit (done with GitHub desktop)\n","2. Install Git Flow with `brew install git-flow`\n","3. Configure the repository with `git flow init`\n","4. Configure feature finishes to be done only in develop with `git config gitflow.feature.finish.keepremote true`\n","5. Synchronize the repository with GDrive\n","6. Read the code from GDrive with Colab\n","\n","**Additional Notes:**\n","\n","* The text mentions using GitHub Desktop, Git Flow, and Colab. These are tools that can be used for version control and code collaboration.\n","* The text also mentions measuring time and memory. This can be done using Python's built-in `time` and `memory_profiler` modules.\n","* English was used for both documentation and code."]},{"cell_type":"markdown","source":["## **Challenge Submission Guidelines:**\n","\n","**Repository:**\n","\n","* Your solution must be in a public repository on the GitHub platform.\n","\n","**Submitting your challenge:**\n","\n","1. Make a POST request to [https://advana-challenge-check-api-cr-k4hdbggvoq-uc.a.run.app/data-engineer](https://advana-challenge-check-api-cr-k4hdbggvoq-uc.a.run.app/data-engineer).\n","2. The request body should be a JSON object with the following fields:\n","    * `name`: Your full name\n","    * `mail`: Your email address\n","    * `github_url`: The URL of your GitHub repository containing the solution\n","\n","**Deadline:**\n","\n","* The deadline for submitting the challenge is 5 calendar days after receiving the challenge.\n","\n","**Technology and Techniques:**\n","\n","* You can use any technology or technique you prefer for data processing.\n","* We will value your knowledge of cloud platforms.\n","* If you use cloud platforms, follow the steps in your files WITHOUT adding access credentials to the different services.\n","\n","**Ranking Criteria:**\n","\n","* Challenges that are clearly organized, explanatory, modular, efficient, and creative will be ranked higher.\n","\n","**Assumptions and Documentation:**\n","\n","* Write down the assumptions you are making.\n","* Include the versions of the libraries you are using in the requirements.txt file.\n","* Do not delete what is already written in the requirements.txt file.\n","* For this challenge, we recommend that you clearly describe how each part of your exercise can be improved.\n","\n","**Data:**\n","\n","* You must use the data contained in the provided file.\n","* You can use the official Twitter documentation to understand the data structure.\n","\n","**Git Usage:**\n","\n","* We will positively evaluate good practices of Git usage.\n","* Use the main branch for any final version you want us to review.\n","* We recommend that you use some GitFlow practice.\n","* Do not delete your development branches.\n","\n","**Error Handling and Edge Cases:**\n","\n","* Consider error handling and edge cases.\n","\n","**Maintainability, Readability, and Scalability:**\n","\n","* Remember that you will be working with other developers, so the maintainability, readability, and scalability of your code is essential.\n","\n","**Code Documentation:**\n","\n","* Good code documentation always helps the reader.\n","\n","**Additional Notes:**\n","\n","* The text mentions using GitHub Desktop, Git Flow, and Colab. These are tools that can be used for version control and code collaboration.\n","* The text also mentions measuring time and memory. This can be done using Python's built-in `time` and `memory_profiler` modules."],"metadata":{"id":"e0ytAOoEE50a"}},{"cell_type":"code","source":["import sys\n","\n","if __name__ != \"__main__\":\n","    sys.exit()\n","\n","# General libraries\n","import logging\n","import os\n","\n","# Data related (if used later)\n","from typing import List, Tuple\n","import datetime\n"],"metadata":{"id":"R45EuZ1JWSa4","executionInfo":{"status":"ok","timestamp":1712342273165,"user_tz":180,"elapsed":558,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## **Definitions and Configuration:**\n","The following code snippet defines constants used in the data transfer and processing pipeline:\n","\n","- **Google Cloud Storage (GCS) Information:**\n","    - `BUCKET_NAME`: Specifies the name of the GCS bucket where data will be uploaded (`tw-gcp-public-lab`).\n","    - `FOLDER_NAME`: Denotes the folder within the bucket to store the uploaded file (`raw`).\n","    - `ZIP_FILE_NAME`: Represents the name of the compressed file containing tweets data (`tweets.json.zip`).\n","    - `GCS_SOURCE_URI`: Constructs the full URI for the file location in GCS after upload (`gs://tw-gcp-public-lab/raw/`).\n","\n","- **Local File Paths:**\n","    - `SOURCE_PATH`: Currently defines a local file path (`/content/drive/Othercomputers/My Mac/latam-challenge`), but it's not used in the provided code for downloading.\n","\n","- **Google Cloud Project and Dataset Information:**\n","    - `PROJECT_ID`: Specifies the Google Cloud project ID (`tw-techdash`).\n","    - `DATASET_NAME`: Defines the name of the BigQuery dataset where the data will be loaded (`tweets_dataset`).\n","    - `TABLE_NAME`: Identifies the name of the BigQuery table to store the extracted tweets data (`tweets`).\n","\n","**Observations:**\n","\n","- The `SOURCE_PATH` might require modification if you intend to download a file from a different location.\n","- Consider using environment variables or a configuration file to manage these constants, making your code more flexible and easier to maintain."],"metadata":{"id":"hfr4Y-Cv8Cd9"}},{"cell_type":"code","source":["# Definitions\n","# Google Cloud Storage (GCS) information\n","BUCKET_NAME: str = os.environ.get(\"BUCKET_NAME\", \"tw-gcp-public-lab\")\n","FOLDER_NAME: str = os.environ.get(\"FOLDER_NAME\", \"raw\")\n","ZIP_FILE_NAME: str = os.environ.get(\"ZIP_FILE_NAME\", \"tweets.json.zip\")\n","FILE_ID: str = os.environ.get(\"FILE_ID\", \"1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis\")\n","GCS_SOURCE_URI: str = os.environ.get(\"GCS_SOURCE_URI\", f\"gs://{BUCKET_NAME}/{FOLDER_NAME}/\")\n","\n","# Local file paths (consider user input/environment variables)\n","MOUNT_POINT: str = os.environ.get(\"MOUNT_POINT\", \"/content/drive\")\n","SOURCE_PATH: str = os.environ.get(\"SOURCE_PATH\", \"/content/drive/Othercomputers/My Mac/latam-challenge\")\n","\n","# Google Cloud project and dataset information (consider environment variables)\n","PROJECT_ID: str = os.environ.get(\"PROJECT_ID\", \"tw-techdash\")\n","DATASET_NAME: str = os.environ.get(\"DATASET_NAME\", \"tweets_dataset\")\n","TABLE_NAME: str = os.environ.get(\"TABLE_NAME\", \"tweets\")\n","\n","# Logging\n","LOGGING_LEVEL: int = os.environ.get(\"LOGGING_LEVEL\", logging.DEBUG)\n","LOGGING_FILE: str = os.environ.get(\"LOGGING_FILE\", \"notebook.log\")\n","\n","# Configuration\n","# Logging\n","logging.basicConfig(filename=LOGGING_FILE, level=LOGGING_LEVEL)"],"metadata":{"id":"g57ic23AMKFp","executionInfo":{"status":"ok","timestamp":1712342273464,"user_tz":180,"elapsed":3,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## **Jupyter Kernel code reloading**\n","\n","**Functionality:**\n","\n","* This code snippet utilizes magic commands within Jupyter Notebooks to manage code reloading.\n","* The `%reload_ext autoreload` line imports and activates the `autoreload` extension.\n","* The `%autoreload 2` line configures the `autoreload` extension to automatically reload Python modules when changes are detected.\n","\n","**Key Concepts:**\n","\n","* **Jupyter Magic Commands:** `%` prefix is used for magic commands that provide special functionality within Jupyter notebooks.\n","* **Autoreload Extension:**  A Jupyter extension that automatically reloads Python modules when changes are detected in the corresponding source files.\n","* **Reload Level:** The level `2` specifies that reload should occur when source files or any imported modules are modified (level 1 only reloads source file changes).\n","\n","**Overall Assessment:**\n","\n","* This code improves development efficiency within Jupyter notebooks by automatically reloading code, avoiding manual restarts.\n","* It leverages the `autoreload` extension for automatic reloading functionality.\n","* The configuration level `2` ensures comprehensive reloading behavior.\n","\n","**Potential Enhancements:**\n","\n","* While automatic reloading is helpful in development, it might not be suitable for production environments due to potential unexpected behavior during execution.\n","* Consider using this approach primarily for interactive development within Jupyter notebooks."],"metadata":{"id":"n0zd65ksIbea"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"fkniVfT9W5hr","executionInfo":{"status":"ok","timestamp":1712342273464,"user_tz":180,"elapsed":2,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}}},"outputs":[],"source":["# Automatically reload the Jupyter kernel when it detects changes in the project directory\n","%reload_ext autoreload\n","\n","# IPython will attempt to automatically reload any module you've previously imported before executing a cell\n","%autoreload 2"]},{"cell_type":"markdown","source":["## **Google Drive mounting**\n","\n","**Functionality:**\n","\n","1. **Connects Google Drive:** This code establishes a connection between your Google Drive storage and the virtual machine running the Colab notebook.\n","2. **Navigates to Project Directory:** This magic command changes the working directory within the Colab notebook to a specific location within your project directory.\n","\n","**Key Concepts:**\n","\n","* **Google Drive Mounting:**\n","    - `from google.colab import drive`: Imports the `drive` module for interacting with Google Drive from Colab.\n","    - `drive.mount('/content/drive', force_remount=True)`: Mounts your Drive at the `/content/drive` path within Colab.\n","    - **Authorization:** Requires initial authorization to grant Colab access to your Drive.\n","* **Jupyter Notebook Magic Commands:**\n","    - `%cd`: A magic command specifically designed for changing directories.\n","\n","**Overall Assessment:**\n","\n","* **Convenient Data Access:** Enables seamless access to your personal data stored in Google Drive for use within Colab notebooks.\n","* **Improved Code Organization:** Helps organize your notebook within the project structure by focusing on a specific subdirectory (like \"src\").\n","\n","**Potential Enhancements:**\n","\n","* **Google Drive Mounting:**\n","    - **Error Handling:** Consider incorporating `try-except` blocks to gracefully handle potential mounting issues.\n","    - **Authentication Persistence:** Explore ways to persist the authentication token (if applicable) to avoid re-authorization for every session.\n","* **Navigation:**\n","    - **Clear Path Definitions:** Replace `{SOURCE_PATH}` with the actual path to your project directory for clarity.\n","    - **Error Handling:** Consider handling potential issues like non-existent directories using Python code (like `try-except` blocks).\n","\n","**Explanation:**\n","\n","1. **Mount Google Drive:** The first part of the code imports the `drive` module and mounts your Google Drive to the `/content/drive` directory within Colab. This allows you to access your Drive files from within your notebook.\n","2. **Change Directory:** The `%cd {SOURCE_PATH}/src` line uses a magic command to navigate to the subdirectory named \"src\" within your project directory (assuming `{SOURCE_PATH}` points to the correct location). This helps organize your notebook by focusing on the relevant project code.\n","\n","**Important Notes:**\n","\n","* Replace `{SOURCE_PATH}` with the actual path to your project directory on your machine.\n","* You'll need to go through an authorization process the first time you run the mounting code to grant Colab access to your Drive."],"metadata":{"id":"KcY78DdKJ1V_"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","# Mount Google Drive with a descriptive mount point name\n","drive.mount(MOUNT_POINT, force_remount=True)\n","\n","# Change directory using a more explicit method\n","os.chdir(os.path.join(MOUNT_POINT, SOURCE_PATH, 'src'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IV-79dMCpYAh","executionInfo":{"status":"ok","timestamp":1712342276653,"user_tz":180,"elapsed":3191,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"374f5c5d-6a59-4a6b-cc3c-c4d2ac9f3cef"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## **Libraries requirements**\n","\n","**Functionality:**\n","\n","- **Installs Python Libraries:** This code snippet installs a collection of Python libraries listed in a file named `requirements.txt` within the currently active virtual environment.\n","\n","**Key Concepts:**\n","\n","- **requirements.txt File:** This text file contains a list of library names and their version requirements, ensuring consistent installation across environments.\n","- **Virtual Environments:** Virtual environments isolate project dependencies, preventing conflicts with other Python projects on your system.\n","- **sys.executable:** This Python variable points to the path of the Python interpreter for the active virtual environment.\n","- **pip:** The Python Package Installer (pip) is used for managing Python packages and libraries.\n","\n","**Explanation:**\n","\n","1. **`import sys`**: Imports the `sys` module, providing access to system-specific variables and functions.\n","2. **`!{sys.executable} -m pip install -r '../requirements.txt'`**: This line calls the pip installer within the virtual environment:\n","   - **`!`**: Jupyter Notebook magic command to execute terminal commands.\n","   - **`{sys.executable}`**: Ensures pip is called from the virtual environment's Python interpreter.\n","   - **`-m`**: Designates a module to execute as a script (in this case, `pip`).\n","   - **`install -r`**: Instructs pip to install packages from a requirements file.\n","   - **`'../requirements.txt'`**: Specifies the path to the requirements file (relative to the current notebook's directory).\n","\n","**Important Notes:**\n","\n","- **Virtual Environment Activation:** Ensure you've activated the desired virtual environment before running this code.\n","- **Path to requirements.txt:** Verify that `../requirements.txt` correctly points to the file's location.\n","- **Internet Connection:** An internet connection is required for pip to download and install packages.\n","\n","**Overall Assessment:**\n","\n","- **Efficient Dependency Management:** Using `requirements.txt` is a best practice for managing project dependencies consistently.\n","- **Consistent Environments:** Facilitates consistent library installations across different machines for reproducibility.\n","- **Collaboration:** Enables easy setup of the same project environment for others.\n","\n","**Potential Enhancements:**\n","\n","- **Error Handling:** Consider incorporating error handling (like try-except blocks) to gracefully handle potential issues during installation, such as network connectivity problems or missing packages.\n"],"metadata":{"id":"KuUJyVVpKQjJ"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"V_d7gErJW5hs","executionInfo":{"status":"ok","timestamp":1712342294218,"user_tz":180,"elapsed":17567,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"47f9cb34-b4fb-47db-b81f-b0b98812824a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully installed libraries from requirements.txt\n"]}],"source":["def install_requirements(\n","    requirements_path: str = \"../requirements.txt\"\n",") -> None:\n","    \"\"\"Installs Python libraries from the specified requirements file.\n","\n","    Args:\n","        requirements_path (str, optional): Path to the requirements.txt file. Defaults to \"../requirements.txt\".\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    import subprocess\n","\n","    try:\n","        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", requirements_path], check=True)\n","        print(\"Successfully installed libraries from requirements.txt\")\n","    except subprocess.CalledProcessError as e:\n","        print(f\"Error installing libraries: {e}\")\n","\n","# Call the install function\n","install_requirements()"]},{"cell_type":"markdown","source":["## **Ingest Google Drive ZIP into Google Storage**\n","**Key Functions:**\n","\n","1. **authenticate_google_drive()**: Authenticates with Google Drive using the user's credentials.\n","2. **download_file_from_drive(drive_service, file_id)**: Downloads a specified file from Google Drive.\n","3. **upload_file_to_cloud_storage(client, bucket_name, folder_name, downloaded, zip_file_name)**: Uploads a file to Google Cloud Storage, handling folder creation if needed.\n","4. **decompress_zip_file(client, bucket_name, folder_name, zip_file_name)**: Decompresses a ZIP file within a GCS bucket.\n","\n","**Code Structure:**\n","\n","- **Logging:** Employs `logging` for debugging and tracking progress.\n","- **Error Handling:** Uses try-except blocks to gracefully handle potential errors.\n","- **Modularity:** Separates functionality into distinct, reusable functions.\n","- **Type Hints:** Enhances code readability and potential type checking.\n","\n","**Main Code Execution:**\n","\n","1. Configures logging to a file named 'transfer.log'.\n","2. Authenticates with Google Drive.\n","3. Downloads the specified file from Drive.\n","4. Creates a Cloud Storage client.\n","5. Uploads the downloaded file to GCS.\n","6. Decompresses the ZIP file in GCS if its content type is 'application/zip'.\n","7. Logs success or failure messages.\n","8. Finally, ensures the downloaded file is closed.\n","\n","**Overall Assessment:**\n","\n","- **Well-structured:** The code is organized, modular, and includes error handling.\n","- **Clear Functionality:** It effectively handles file transfer and decompression tasks.\n","- **Authentication Flexibility:** Uses authentication methods external to the code (useful for avoiding credentials in code).\n","- **Good Practices:** Adheres to good practices like logging and try-except blocks.\n","\n","**Potential Enhancements:**\n","\n","- **Parameterization:** Explore using command-line arguments or configuration files to adjust parameters more flexibly.\n","- **Progress Reporting:** Consider more granular progress reporting for downloads/uploads.\n","- **Content Validation:** Validate file content after decompression for integrity.\n","- **Advanced Error Handling:** Implement retries or alternative actions for potential errors.\n","\n","This code provides a foundation for file transfer and decompression tasks within Google Cloud environments, demonstrating clarity and attention to best practices."],"metadata":{"id":"PA2wPhkO7khO"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"pz4EZrB4W5hu","outputId":"a21f1737-1ae5-4d5e-de10-03ea2ef24e09","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712342312217,"user_tz":180,"elapsed":18003,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading 100%\n","File uploaded to gs://tw-gcp-public-lab/raw/tweets.json.zip\n","File decompressed in gs://tw-gcp-public-lab/raw/farmers-protest-tweets-2021-2-4.json\n","File transfer process completed.\n"]}],"source":["from typing import Optional\n","from google.colab import auth\n","from googleapiclient.discovery import build\n","import io\n","from googleapiclient.http import MediaIoBaseDownload\n","from google.cloud import storage\n","import zipfile\n","\n","\n","def authenticate_google_drive() -> None:\n","    \"\"\"Authenticates to Google Drive using the user's credentials.\n","\n","    Args:\n","        None\n","    \"\"\"\n","    try:\n","        auth.authenticate_user()\n","    except Exception as e:\n","        logging.error(f\"Error authenticating to Google Drive: {e}\")\n","        raise\n","\n","\n","def download_file_from_drive(\n","    drive_service: build, file_id: str\n",") -> io.BytesIO:\n","    \"\"\"Downloads a file from Google Drive.\n","\n","    Args:\n","        drive_service (googleapiclient.discovery.Resource): The Google Drive service resource.\n","        file_id (str): The ID of the file to download.\n","\n","    Returns:\n","        io.BytesIO: The downloaded file content as a BytesIO object.\n","    \"\"\"\n","    request = drive_service.files().get_media(fileId=file_id)\n","    downloaded = io.BytesIO()\n","    downloader = MediaIoBaseDownload(downloaded, request)\n","    done = False\n","    while not done:\n","        status, done = downloader.next_chunk()\n","        print(f'Downloading {int(status.progress() * 100)}%')\n","    downloaded.seek(0)\n","    return downloaded\n","\n","\n","def upload_file_to_cloud_storage(\n","    client: storage.Client, bucket_name: str, folder_name: str, downloaded: io.BytesIO, zip_file_name: str\n",") -> storage.blob.Blob:\n","    \"\"\"Uploads a file to Google Cloud Storage.\n","\n","    Args:\n","        client (google.cloud.storage.Client): The Google Cloud Storage client.\n","        bucket_name (str): The name of the Google Cloud Storage bucket.\n","        folder_name (str): The name of the folder within the bucket where the file will be uploaded.\n","        downloaded (io.BytesIO): The downloaded file to upload.\n","        zip_file_name (str): The name of the file to be uploaded.\n","\n","    Returns:\n","        google.cloud.storage.Blob: The uploaded blob object.\n","    \"\"\"\n","    bucket = client.bucket(bucket_name)\n","    folder_blob = bucket.blob(f\"{folder_name}/\")\n","\n","    # Check and create folder if it doesn't exist\n","    if not folder_blob.exists():\n","        folder_blob.upload_from_string('', content_type='application/x-www-form-urlencoded;charset=UTF-8')\n","\n","    # Upload the file to the specified folder\n","    blob = bucket.blob(f'{folder_name}/{zip_file_name}')\n","    blob.upload_from_file(downloaded, content_type='application/zip')\n","\n","    print(f'File uploaded to gs://{bucket_name}/{blob.name}')\n","    return blob\n","\n","\n","def decompress_zip_file(\n","    client: storage.Client, bucket_name: str, folder_name: str, zip_file_name: str\n",") -> str:\n","    \"\"\"Decompresses a ZIP file stored in Google Cloud Storage.\n","\n","    Args:\n","        client (google.cloud.storage.Client): The Google Cloud Storage client.\n","        bucket_name (str): The name of the bucket where the ZIP file is stored.\n","        folder_name (str): The name of the folder within the bucket where the ZIP file is located.\n","\n","    Returns:\n","        str: The unzipped file name.\n","    \"\"\"\n","    try:\n","        bucket = client.bucket(bucket_name)\n","        blob = bucket.blob(f'{folder_name}/{zip_file_name}')\n","        with zipfile.ZipFile(io.BytesIO(blob.download_as_string()), 'r') as z:\n","            for file_info in z.infolist():\n","                with z.open(file_info) as file:\n","                    blob_name = f'{folder_name}/{file_info.filename}'\n","                    json_file_name = file_info.filename\n","                    blob = bucket.blob(blob_name)\n","                    blob.upload_from_file(file)\n","        print(f'File decompressed in gs://{bucket_name}/{blob_name}')\n","    except zipfile.BadZipFile:\n","        logging.warning(f'The file in gs://{bucket_name}/{folder_name}/{zip_file_name} is not a valid ZIP file.')\n","    except Exception as e:\n","        logging.error(f'Error decompressing file: {e}')\n","    finally:\n","        return json_file_name\n","\n","try:\n","    # Authenticate to Google Drive\n","    authenticate_google_drive()\n","    drive_service = build('drive', 'v3')\n","\n","    # Download file from Drive\n","    downloaded = download_file_from_drive(drive_service, FILE_ID)\n","\n","    # Create Cloud Storage client\n","    storage_client = storage.Client()\n","\n","    # Upload file to Cloud Storage\n","    uploaded_blob = upload_file_to_cloud_storage(storage_client, BUCKET_NAME, FOLDER_NAME, downloaded, ZIP_FILE_NAME)\n","\n","    # Decompress ZIP file if applicable\n","    if uploaded_blob.content_type == 'application/zip':\n","        json_file_name = decompress_zip_file(storage_client, BUCKET_NAME, FOLDER_NAME, ZIP_FILE_NAME)\n","\n","    logging.info(\"File transfer successful!\")\n","\n","except Exception as e:\n","    logging.error(f\"An error occurred: {e}\")\n","\n","finally:\n","    downloaded.close()  # Close downloaded file\n","    print(\"File transfer process completed.\")\n"]},{"cell_type":"markdown","source":["## **Store JSON into BigQuery**\n","\n","**Key Functions:**\n","\n","1. **authenticate_bigquery()**: Authenticates with BigQuery and returns a client object.\n","2. **create_dataset(client, dataset_name, mode)**: Creates a dataset, handling overwrite options.\n","3. **create_table(client, dataset_name, table_name, mode)**: Creates a table, handling overwrite options and using schema inference.\n","4. **load_data_from_storage(client, source_uri, dataset_name, table_name)**: Loads newline-delimited JSON data from GCS to a BigQuery table, auto-detecting schema and ignoring unknown values.\n","\n","**Code Structure:**\n","\n","- **Clear Function Definitions:** Each function has a specific purpose with descriptive names.\n","- **Error Handling:** Uses `try-except` blocks to handle potential errors gracefully.\n","- **Parameterization:** Functions allow for customization of dataset, table, and source URI details.\n","- **Concise Comments:** Comments clarify intent without excessive detail.\n","\n","**Main Code Execution:**\n","\n","1. Authenticates with BigQuery.\n","2. Creates the dataset, overwriting if it exists.\n","3. Creates the table, overwriting if it exists.\n","4. Loads data from Cloud Storage into the BigQuery table.\n","5. Prints a completion message.\n","\n","**Key Points:**\n","\n","- **Schema Inference:** Relies on BigQuery to infer the schema during table creation.\n","- **Handles Unknown Values:** Ignores unknown values during data loading, potentially simplifying error handling.\n","- **Waits for Load Completion:** Ensures the loading job completes before proceeding.\n","\n","**Potential Enhancements:**\n","\n","- **Explicit Schema Definition:** Consider defining the schema explicitly for more control and consistency.\n","- **Data Validation:** Incorporate validation steps to ensure data quality and integrity.\n","- **Advanced Error Handling:** Implement retry mechanisms or logging for load job failures.\n","- **Logging:** Add logging for better tracking and debugging.\n","\n","**Overall:**\n","\n","The code demonstrates a well-structured and functional approach to interacting with BigQuery for dataset creation, table setup, and data loading. It incorporates practical techniques for schema inference and unknown value handling, while maintaining clarity and error handling."],"metadata":{"id":"Dh8NblbcCU8H"}},{"cell_type":"code","source":["from google.cloud import bigquery\n","from google.api_core.exceptions import NotFound\n","\n","\n","def authenticate_bigquery(\n","    project_id: str\n",") -> bigquery.Client:\n","    \"\"\"Authenticates to BigQuery and returns the client object.\"\"\"\n","    return bigquery.Client(project_id)\n","\n","\n","def create_dataset(\n","    client: bigquery.Client, dataset_name: str, mode: Optional[str] = 'create'\n",") -> None:\n","    \"\"\"\n","    Creates a BigQuery dataset if it doesn't exist.\n","\n","    Args:\n","        client: BigQuery client object.\n","        dataset_name: Name of the dataset to create.\n","        mode: Action to take if the dataset already exists ('create' or 'overwrite').\n","    \"\"\"\n","\n","    dataset_ref = client.dataset(dataset_name)\n","    try:\n","        client.get_dataset(dataset_ref)\n","        if mode == 'overwrite':\n","            client.delete_dataset(dataset_ref, delete_contents=True)\n","            client.create_dataset(dataset_ref)\n","            print(f\"Dataset '{dataset_name}' overwritten.\")\n","        else:\n","            print(f\"Dataset '{dataset_name}' already exists.\")\n","    except NotFound:\n","        client.create_dataset(dataset_ref)\n","        print(f\"Dataset '{dataset_name}' created.\")\n","\n","\n","def create_table(\n","    client: bigquery.Client, dataset_name: str, table_name: str, mode: Optional[str] = 'create'\n",") -> None:\n","    \"\"\"\n","    Creates a BigQuery table if it doesn't exist.\n","\n","    Args:\n","        client: BigQuery client object.\n","        dataset_name: Name of the dataset containing the table.\n","        table_name: Name of the table to create.\n","        mode: Action to take if the table already exists ('create' or 'overwrite').\n","    \"\"\"\n","\n","    dataset_ref = client.dataset(dataset_name)\n","    table_ref = dataset_ref.table(table_name)\n","    try:\n","        client.get_table(table_ref)\n","        print(f\"Table '{table_name}' already exists.\")\n","        if mode == 'overwrite':\n","            client.delete_table(table_ref)\n","            table = bigquery.Table(table_ref)\n","            table.schema = []  # Empty schema for BigQuery to infer\n","            client.create_table(table)\n","            print(f\"Table '{table_name}' overwritten.\")\n","    except NotFound:\n","        table = bigquery.Table(table_ref)\n","        table.schema = []  # Empty schema for BigQuery to infer\n","        client.create_table(table)\n","        print(f\"Table '{table_name}' created.\")\n","\n","\n","def load_data_from_storage(\n","    client: bigquery.Client, source_uri: str, dataset_name: str, table_name: str\n",") -> None:\n","    \"\"\"\n","    Loads data from Cloud Storage (newline-delimited JSON) to BigQuery table.\n","\n","    Args:\n","        client: BigQuery client object.\n","        source_uri: URI of the data file in Cloud Storage.\n","        dataset_name: Name of the dataset containing the table.\n","        table_name: Name of the table to load data into.\n","    \"\"\"\n","\n","    job_config = bigquery.LoadJobConfig()\n","    job_config.source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON\n","    job_config.autodetect = True  # Auto-detect schema\n","    job_config.ignore_unknown_values = True  # Ignore unknown values\n","\n","    load_job = client.load_table_from_uri(\n","        source_uri + json_file_name,\n","        client.dataset(dataset_name).table(table_name),\n","        job_config=job_config\n","    )\n","    load_job.result()  # Wait for load completion\n","\n","\n","# Authenticate to BigQuery\n","bigquery_client = authenticate_bigquery(PROJECT_ID)\n","\n","# Create dataset (overwrite if needed)\n","create_dataset(bigquery_client, DATASET_NAME, mode='overwrite')\n","\n","# Create table (overwrite if needed)\n","create_table(bigquery_client, DATASET_NAME, TABLE_NAME, mode='overwrite')\n","\n","# Load data from Cloud Storage\n","load_data_from_storage(bigquery_client, GCS_SOURCE_URI, DATASET_NAME, TABLE_NAME)\n","\n","print(\"Data loading completed!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"teRz4l7jW-fq","executionInfo":{"status":"ok","timestamp":1712342336492,"user_tz":180,"elapsed":24288,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"67bfc258-1544-4237-c419-d79101b05db3"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset 'tweets_dataset' overwritten.\n","Table 'tweets' created.\n","Data loading completed!\n"]}]},{"cell_type":"markdown","source":["## **Processing Data with BigQuery**\n","\n","**Functionality:**\n","\n","- **Constructs BigQuery Query:** The `construct_bigquery_query` function builds a formatted SQL query string to fetch user data based on specific criteria.\n","- **Processes BigQuery Results:** The `process_bigquery_results` function executes the query, handles results, and converts them into a desired format (list of tuples with date and username).\n","\n","**Key Concepts:**\n","\n","- **Type Hints:** Employs type hints (`List`, `Tuple`, `datetime.date`) for improved code readability and potential static type checking.\n","- **Error Handling:** Incorporates `try-except` blocks to gracefully handle exceptions (`BadRequest` and generic exceptions).\n","- **Data Conversion:** Converts retrieved data rows into the specified format.\n","\n","**Overall Assessment:**\n","\n","- **Clear Separation:** Functions promote modularity and reusability.\n","- **Meaningful Variable Names:** Descriptive names enhance code understandability.\n","- **Error Management:** Handles potential errors during query execution and processing.\n","\n","**Potential Enhancements:**\n","\n","- **Input Validation:** Consider validating the constructed query string before execution.\n","- **Logging:** Integrate logging for detailed tracking and debugging.\n","- **Security:** Ensure secure credential management for BigQuery access.\n","- **Query Parameterization:** If DATASET_NAME and TABLE_NAME are not intended for hardcoding, utilize BigQuery's query parameters for better reusability and security.\n","- **Data Usage:** Currently, the extracted data is printed. You can modify this section to store the data in a desired location or perform further processing.\n","\n","This code provides a foundation for working with BigQuery data retrieval and basic processing. You can extend it based on your specific needs."],"metadata":{"id":"dEcJqRweD26B"}},{"cell_type":"code","source":["from google.api_core.exceptions import BadRequest\n","\n","def construct_bigquery_query(\n","    dataset: str, table: str\n",") -> str:\n","    \"\"\"\n","    Constructs a BigQuery SQL query to retrieve user data.\n","\n","    Args:\n","        dataset: Name of the BigQuery dataset containing the table.\n","        table: Name of the table to query.\n","\n","    Returns:\n","        The formatted BigQuery SQL query string.\n","    \"\"\"\n","\n","    query = f\"\"\"\n","      SELECT\n","          CAST(date AS DATE) AS date,\n","          IFNULL(CAST(user.username AS STRING), '') AS username\n","      FROM `{dataset}.{table}`\n","      WHERE user IS NOT NULL\n","          AND user.profileBannerUrl IS NOT NULL\n","      LIMIT 3\n","    \"\"\"\n","\n","    return query\n","\n","\n","def process_bigquery_results(\n","    client: bigquery.Client, query: str\n",") -> List[Tuple[datetime.date, str]]:\n","    \"\"\"\n","    Executes a BigQuery query, handles results, and performs data conversion.\n","\n","    Args:\n","        client: BigQuery client object.\n","        query: BigQuery SQL query string.\n","\n","    Returns:\n","        A list of tuples containing the extracted data (date and username).\n","\n","    Raises:\n","        Exception: For any unexpected errors during query execution or processing.\n","    \"\"\"\n","\n","    try:\n","        query_job = bigquery_client.query(query)\n","        results = query_job.result()\n","\n","        if not results:\n","            print(\"No results found for the query.\")\n","            return []  # Return empty list for consistency\n","\n","        extracted_data = [(row[0], str(row[1])) for row in results]\n","        return extracted_data\n","\n","    except BadRequest as e:\n","        print(f\"BigQuery error: {e}\")\n","        raise\n","    except Exception as e:\n","        print(f\"Error: {e}\")\n","        raise\n","\n","try:\n","    # Construct and execute the query\n","    query = construct_bigquery_query(DATASET_NAME, TABLE_NAME)\n","    extracted_data = process_bigquery_results(bigquery_client, query)\n","\n","    # Handle the extracted data (e.g., print, store in a database)\n","    print(extracted_data)\n","\n","except Exception as e:\n","    print(f\"Unexpected error: {e}\")\n"],"metadata":{"id":"WLlwchwVZqYx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712342337532,"user_tz":180,"elapsed":1043,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}},"outputId":"e5639ec0-f628-458f-b9a1-db832723335e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[(datetime.date(2021, 2, 14), 'amiiigill'), (datetime.date(2021, 2, 14), 'KalamDeep'), (datetime.date(2021, 2, 14), 'mikhan700')]\n"]}]},{"cell_type":"code","source":["# TODO: check this\n","#import q1_time\n","\n","# Llama a la función q1_time para obtener el resultado\n","#resultado = q1_time.q1_time(file_path)\n","\n","# Imprime el resultado\n","#print(resultado)"],"metadata":{"id":"5oTEJKuIMCvP","executionInfo":{"status":"ok","timestamp":1712342337532,"user_tz":180,"elapsed":3,"user":{"displayName":"Juan Herrera","userId":"01250943537373452870"}}},"execution_count":9,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}